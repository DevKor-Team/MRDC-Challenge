{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPQa4GjgqhAP8YeH9Pn1FTa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f607a97a9f61430da604dd03bab5444f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5662be5e09f44af98a1977260635569d",
              "IPY_MODEL_31fd70e65e4a49038fd1d94c654f6306",
              "IPY_MODEL_bdd9d7d7dca34ead8b02ffab68628ebc"
            ],
            "layout": "IPY_MODEL_9dd66476cb804f4c959243e61b144c03"
          }
        },
        "5662be5e09f44af98a1977260635569d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b26a6aefc7c4c0eb84c2f9359f067c2",
            "placeholder": "​",
            "style": "IPY_MODEL_a4e4b5c65beb4301864130376bae0a44",
            "value": "100%"
          }
        },
        "31fd70e65e4a49038fd1d94c654f6306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8407415369aa47daae1a5e1cfdb4ca47",
            "max": 100488385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e73822bf9f2a4ce5930f60c53da77339",
            "value": 100488385
          }
        },
        "bdd9d7d7dca34ead8b02ffab68628ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbd76149d0874015b13fa0fe4b09e614",
            "placeholder": "​",
            "style": "IPY_MODEL_6ee5e0a848a047d7a0328620a92b10bf",
            "value": " 95.8M/95.8M [00:02&lt;00:00, 59.0MB/s]"
          }
        },
        "9dd66476cb804f4c959243e61b144c03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b26a6aefc7c4c0eb84c2f9359f067c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4e4b5c65beb4301864130376bae0a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8407415369aa47daae1a5e1cfdb4ca47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e73822bf9f2a4ce5930f60c53da77339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbd76149d0874015b13fa0fe4b09e614": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ee5e0a848a047d7a0328620a92b10bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevKor-Team/MRDC-Challenge/blob/jo/jo/week4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL4-pykT9LB9",
        "outputId": "98821481-6821-4703-eec5-4d5992c09ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools    # confusion matrix에서 사용\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import albumentations\n",
        "import albumentations.pytorch"
      ],
      "metadata": {
        "id": "5prj8-l_CZP7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/content/gdrive/MyDrive/ML/MRDC-competition/data'\n",
        "\n",
        "import os\n",
        "\n",
        "train = pd.read_csv(os.path.join(BASE_DIR, 'Train.csv'))\n",
        "print(train.shape)\n",
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "_jKogXDPCirT",
        "outputId": "79c7eec0-35ec-458d-c73c-820ce6df4a5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5340, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Image_id  Label\n",
              "0      id_004wknd7qd.jpg  blast\n",
              "1  id_004wknd7qd_rgn.jpg  blast\n",
              "2      id_005sitfgr2.jpg  brown\n",
              "3  id_005sitfgr2_rgn.jpg  brown\n",
              "4      id_00stp9t6m6.jpg  blast"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-554c1d5f-a93e-4656-9a83-57ef5afb7451\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_id</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_004wknd7qd.jpg</td>\n",
              "      <td>blast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_004wknd7qd_rgn.jpg</td>\n",
              "      <td>blast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_005sitfgr2.jpg</td>\n",
              "      <td>brown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_005sitfgr2_rgn.jpg</td>\n",
              "      <td>brown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_00stp9t6m6.jpg</td>\n",
              "      <td>blast</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-554c1d5f-a93e-4656-9a83-57ef5afb7451')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-554c1d5f-a93e-4656-9a83-57ef5afb7451 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-554c1d5f-a93e-4656-9a83-57ef5afb7451');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_rgb = train.loc[~train['Image_id'].str.contains('_rgn')]\n",
        "train_rgb.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BlZTyElyCn4G",
        "outputId": "7be60a6f-5e15-401e-9e57-6e26a59b303c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Image_id    Label\n",
              "0  id_004wknd7qd.jpg    blast\n",
              "2  id_005sitfgr2.jpg    brown\n",
              "4  id_00stp9t6m6.jpg    blast\n",
              "6  id_012zxewnhx.jpg    blast\n",
              "8  id_0186qwq2at.jpg  healthy"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2d5fa23-4f00-4231-b9a2-a8d80cb8b912\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_id</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_004wknd7qd.jpg</td>\n",
              "      <td>blast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_005sitfgr2.jpg</td>\n",
              "      <td>brown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_00stp9t6m6.jpg</td>\n",
              "      <td>blast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>id_012zxewnhx.jpg</td>\n",
              "      <td>blast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>id_0186qwq2at.jpg</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2d5fa23-4f00-4231-b9a2-a8d80cb8b912')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e2d5fa23-4f00-4231-b9a2-a8d80cb8b912 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e2d5fa23-4f00-4231-b9a2-a8d80cb8b912');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(os.path.join(BASE_DIR, 'Test.csv'))\n",
        "test_rgb = test.loc[~test['Image_id'].str.contains('_rgn')]\n",
        "test_rgb.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wT-Je5OpCobm",
        "outputId": "65a34e9f-51b9-42b3-b9c3-96fe595521c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Image_id\n",
              "0  id_00vl5wvxq3.jpg\n",
              "2  id_01hu05mtch.jpg\n",
              "4  id_030ln10ewn.jpg\n",
              "6  id_03z57m8xht.jpg\n",
              "8  id_04ngep1w4b.jpg"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f07cedf-4a42-4597-b054-af4e70dc25b7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_00vl5wvxq3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_01hu05mtch.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_030ln10ewn.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>id_03z57m8xht.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>id_04ngep1w4b.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f07cedf-4a42-4597-b054-af4e70dc25b7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f07cedf-4a42-4597-b054-af4e70dc25b7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f07cedf-4a42-4597-b054-af4e70dc25b7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ss = pd.read_csv(os.path.join(BASE_DIR, 'SampleSubmission.csv'))\n",
        "ss.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qvm5UuNiCwRZ",
        "outputId": "9378f06e-b4db-4d2b-bdc1-a08f6fd2f73b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Image_id  blast  brown  healthy\n",
              "0  id_00vl5wvxq3.jpg    0.0    0.0      0.0\n",
              "1  id_01hu05mtch.jpg    0.0    0.0      0.0\n",
              "2  id_030ln10ewn.jpg    0.0    0.0      0.0\n",
              "3  id_03z57m8xht.jpg    0.0    0.0      0.0\n",
              "4  id_04ngep1w4b.jpg    0.0    0.0      0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8cd78049-bb87-4b76-ac3f-da8094f77910\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_id</th>\n",
              "      <th>blast</th>\n",
              "      <th>brown</th>\n",
              "      <th>healthy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_00vl5wvxq3.jpg</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_01hu05mtch.jpg</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_030ln10ewn.jpg</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_03z57m8xht.jpg</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_04ngep1w4b.jpg</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8cd78049-bb87-4b76-ac3f-da8094f77910')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8cd78049-bb87-4b76-ac3f-da8094f77910 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8cd78049-bb87-4b76-ac3f-da8094f77910');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): validation loss가 개선된 후 기다리는 기간\n",
        "                            Default: 7\n",
        "            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n",
        "                            Default: False\n",
        "            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n",
        "                            Default: 0\n",
        "            path (str): checkpoint저장 경로\n",
        "                            Default: 'checkpoint.pt'\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''validation loss가 감소하면 모델을 저장한다.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "ZHJqG41OCyV7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(device, model, train_loader, val_loader, criterion, optimizer, num_epochs=5, early_stopping=None):\n",
        "    model = model.to(device)\n",
        "\n",
        "    dl = {'train': train_loader,\n",
        "          'val': val_loader}\n",
        "    \n",
        "    val_label = []\n",
        "    val_pred = []\n",
        "\n",
        "    val_loss = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        val_label = []\n",
        "        val_pred = []\n",
        "\n",
        "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dl[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients -> backward시 필요\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    # print(preds)\n",
        "                    # print(labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    if phase == 'val':    # and epoch == num_epochs - 1:\n",
        "                        val_label += labels.tolist()\n",
        "                        val_pred += preds.tolist()\n",
        "                        # print(val_label)\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)  # size(0) : batch size (첫 번째 차원 개수)\n",
        "                                                              # item() : tensor에서 저장된 값만 가져오기\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dl[phase].dataset)  # 이렇게 나누면 epoch당 평균 loss가 됨\n",
        "            epoch_acc = running_corrects.double() / len(dl[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "            val_loss = epoch_loss\n",
        "\n",
        "        if early_stopping != None:\n",
        "          early_stopping(val_loss, model) # epoch_loss에는 validation loss가 저장\n",
        "          if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(torch.load(early_stopping.path))\n",
        "    return model, val_label, val_pred"
      ],
      "metadata": {
        "id": "VKSCKoXZCz-Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(device, model, test_loader):\n",
        "    test_pred = []\n",
        "\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for features in test_loader:\n",
        "            features = features.to(device)\n",
        "\n",
        "            outputs = model(features.to(torch.float))\n",
        "            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "            test_pred.append(probabilities.tolist())\n",
        "\n",
        "    return test_pred"
      ],
      "metadata": {
        "id": "p6T95w0bDB1b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix 시각화\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(cm, target_names=None, labels=True):\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "\n",
        "    cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    thresh = cm.max() / 2\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if labels:\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]), horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i,j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-pnsMxqiDC7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Img_Dataset_with_aug(Dataset):\n",
        "    def __init__(self, file_path, aug_transform, table, is_train=True, test_transform=None):\n",
        "        self.file_path = file_path\n",
        "        self.aug_transform = aug_transform\n",
        "        self.table = table\n",
        "        self.is_train = is_train\n",
        "        self.test_transform = test_transform\n",
        "        \n",
        "        self.img_name_list = self.table['Image_id'].tolist()\n",
        "        self.img_list = []\n",
        "\n",
        "        if self.is_train:   # train -> augmentation 필요\n",
        "          self.table_label_list = self.table['Label'].tolist()\n",
        "          self.label_list = []\n",
        "          for i, img_name in enumerate(self.img_name_list):\n",
        "            img = Image.open(os.path.join(self.file_path, img_name))\n",
        "            print(i)\n",
        "            if self.table_label_list[i] == 'blast':\n",
        "              img_transformed = self.test_transform(image=np.array(img))['image']\n",
        "              self.img_list.append(img_transformed)\n",
        "              self.label_list.append(0)\n",
        "            elif self.table_label_list[i] == 'brown':\n",
        "              for _ in range(2):   \n",
        "                img_transformed = self.aug_transform(image=np.array(img))['image']\n",
        "                self.img_list.append(img_transformed)\n",
        "                self.label_list.append(1)\n",
        "            else:\n",
        "              for _ in range(4):   \n",
        "                img_transformed = self.aug_transform(image=np.array(img))['image']\n",
        "                self.img_list.append(img_transformed)\n",
        "                self.label_list.append(2)\n",
        "\n",
        "        else:   # test -> augmentation 필요X\n",
        "            for img_name in self.img_name_list:\n",
        "                img = Image.open(os.path.join(self.file_path, img_name))\n",
        "                img_transformed = self.test_transform(image=np.array(img))['image']\n",
        "                self.img_list.append(img_transformed)\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        if self.is_train:\n",
        "            return self.img_list[index], self.label_list[index]\n",
        "        else:\n",
        "            return self.img_list[index]"
      ],
      "metadata": {
        "id": "ZRNjEWMDDGgT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_data_loader_with_aug(batch_size=128, size=224, split=0.8):\n",
        "  IMG_DIR = '/content/gdrive/MyDrive/ML/MRDC-competition/data/images'\n",
        "  aug_transform=albumentations.Compose([\n",
        "  albumentations.Resize(size, size),\n",
        "  albumentations.OneOf([\n",
        "                        albumentations.HorizontalFlip(p=1),\n",
        "                        albumentations.RandomRotate90(p=1),\n",
        "                        albumentations.VerticalFlip(p=1)            \n",
        "  ], p=0.5),\n",
        "  albumentations.Normalize(),\n",
        "  albumentations.pytorch.transforms.ToTensorV2(),\n",
        "])\n",
        "  test_transform=albumentations.Compose([\n",
        "    albumentations.Resize(size, size),\n",
        "    albumentations.Normalize(),\n",
        "    albumentations.pytorch.transforms.ToTensorV2(),\n",
        "  ])\n",
        "\n",
        "  # train data\n",
        "  train_dataset = Img_Dataset_with_aug(IMG_DIR, aug_transform, train_rgb, test_transform=test_transform)\n",
        "\n",
        "  train_size = int(len(train_dataset) * split)\n",
        "  val_size = len(train_dataset) - train_size\n",
        "\n",
        "  train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)\n",
        "  \n",
        "  # test data\n",
        "  test_dataset = Img_Dataset_with_aug(IMG_DIR, aug_transform, test_rgb, is_train=False, test_transform=test_transform)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "  \n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "H1ZTTYoHDHU2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cuda out of memory error 대비\n",
        "# 이거 한 번 돌리고 batch size 줄여서 위의 셀 다시 한 번 돌려주기\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "oUtVEU_2XteB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConvNext-Base 384"
      ],
      "metadata": {
        "id": "Nbge0A-DDJYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# check whether run in Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install timm==0.5.4 \n",
        "\n",
        "from timm import create_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtRMMQ9oFWxL",
        "outputId": "66bfb87b-fd28-469f-887d-6aa4601ef067"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm==0.5.4\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm==0.5.4) (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm==0.5.4) (0.13.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm==0.5.4) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.5.4) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.5.4) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.5.4) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.5.4) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.5.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.5.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.5.4) (2.10)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "avail_pretrained_models = timm.list_models(pretrained=True)\n",
        "len(avail_pretrained_models), avail_pretrained_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WW87JvDOMJg",
        "outputId": "3c349273-e623-4e20-cdff-cb51ab7d87ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(592,\n",
              " ['adv_inception_v3',\n",
              "  'bat_resnext26ts',\n",
              "  'beit_base_patch16_224',\n",
              "  'beit_base_patch16_224_in22k',\n",
              "  'beit_base_patch16_384',\n",
              "  'beit_large_patch16_224',\n",
              "  'beit_large_patch16_224_in22k',\n",
              "  'beit_large_patch16_384',\n",
              "  'beit_large_patch16_512',\n",
              "  'botnet26t_256',\n",
              "  'cait_m36_384',\n",
              "  'cait_m48_448',\n",
              "  'cait_s24_224',\n",
              "  'cait_s24_384',\n",
              "  'cait_s36_384',\n",
              "  'cait_xs24_384',\n",
              "  'cait_xxs24_224',\n",
              "  'cait_xxs24_384',\n",
              "  'cait_xxs36_224',\n",
              "  'cait_xxs36_384',\n",
              "  'coat_lite_mini',\n",
              "  'coat_lite_small',\n",
              "  'coat_lite_tiny',\n",
              "  'coat_mini',\n",
              "  'coat_tiny',\n",
              "  'convit_base',\n",
              "  'convit_small',\n",
              "  'convit_tiny',\n",
              "  'convmixer_768_32',\n",
              "  'convmixer_1024_20_ks9_p14',\n",
              "  'convmixer_1536_20',\n",
              "  'convnext_base',\n",
              "  'convnext_base_384_in22ft1k',\n",
              "  'convnext_base_in22ft1k',\n",
              "  'convnext_base_in22k',\n",
              "  'convnext_large',\n",
              "  'convnext_large_384_in22ft1k',\n",
              "  'convnext_large_in22ft1k',\n",
              "  'convnext_large_in22k',\n",
              "  'convnext_small',\n",
              "  'convnext_tiny',\n",
              "  'convnext_xlarge_384_in22ft1k',\n",
              "  'convnext_xlarge_in22ft1k',\n",
              "  'convnext_xlarge_in22k',\n",
              "  'crossvit_9_240',\n",
              "  'crossvit_9_dagger_240',\n",
              "  'crossvit_15_240',\n",
              "  'crossvit_15_dagger_240',\n",
              "  'crossvit_15_dagger_408',\n",
              "  'crossvit_18_240',\n",
              "  'crossvit_18_dagger_240',\n",
              "  'crossvit_18_dagger_408',\n",
              "  'crossvit_base_240',\n",
              "  'crossvit_small_240',\n",
              "  'crossvit_tiny_240',\n",
              "  'cspdarknet53',\n",
              "  'cspresnet50',\n",
              "  'cspresnext50',\n",
              "  'deit_base_distilled_patch16_224',\n",
              "  'deit_base_distilled_patch16_384',\n",
              "  'deit_base_patch16_224',\n",
              "  'deit_base_patch16_384',\n",
              "  'deit_small_distilled_patch16_224',\n",
              "  'deit_small_patch16_224',\n",
              "  'deit_tiny_distilled_patch16_224',\n",
              "  'deit_tiny_patch16_224',\n",
              "  'densenet121',\n",
              "  'densenet161',\n",
              "  'densenet169',\n",
              "  'densenet201',\n",
              "  'densenetblur121d',\n",
              "  'dla34',\n",
              "  'dla46_c',\n",
              "  'dla46x_c',\n",
              "  'dla60',\n",
              "  'dla60_res2net',\n",
              "  'dla60_res2next',\n",
              "  'dla60x',\n",
              "  'dla60x_c',\n",
              "  'dla102',\n",
              "  'dla102x',\n",
              "  'dla102x2',\n",
              "  'dla169',\n",
              "  'dm_nfnet_f0',\n",
              "  'dm_nfnet_f1',\n",
              "  'dm_nfnet_f2',\n",
              "  'dm_nfnet_f3',\n",
              "  'dm_nfnet_f4',\n",
              "  'dm_nfnet_f5',\n",
              "  'dm_nfnet_f6',\n",
              "  'dpn68',\n",
              "  'dpn68b',\n",
              "  'dpn92',\n",
              "  'dpn98',\n",
              "  'dpn107',\n",
              "  'dpn131',\n",
              "  'eca_botnext26ts_256',\n",
              "  'eca_halonext26ts',\n",
              "  'eca_nfnet_l0',\n",
              "  'eca_nfnet_l1',\n",
              "  'eca_nfnet_l2',\n",
              "  'eca_resnet33ts',\n",
              "  'eca_resnext26ts',\n",
              "  'ecaresnet26t',\n",
              "  'ecaresnet50d',\n",
              "  'ecaresnet50d_pruned',\n",
              "  'ecaresnet50t',\n",
              "  'ecaresnet101d',\n",
              "  'ecaresnet101d_pruned',\n",
              "  'ecaresnet269d',\n",
              "  'ecaresnetlight',\n",
              "  'efficientnet_b0',\n",
              "  'efficientnet_b1',\n",
              "  'efficientnet_b1_pruned',\n",
              "  'efficientnet_b2',\n",
              "  'efficientnet_b2_pruned',\n",
              "  'efficientnet_b3',\n",
              "  'efficientnet_b3_pruned',\n",
              "  'efficientnet_b4',\n",
              "  'efficientnet_el',\n",
              "  'efficientnet_el_pruned',\n",
              "  'efficientnet_em',\n",
              "  'efficientnet_es',\n",
              "  'efficientnet_es_pruned',\n",
              "  'efficientnet_lite0',\n",
              "  'efficientnetv2_rw_m',\n",
              "  'efficientnetv2_rw_s',\n",
              "  'efficientnetv2_rw_t',\n",
              "  'ens_adv_inception_resnet_v2',\n",
              "  'ese_vovnet19b_dw',\n",
              "  'ese_vovnet39b',\n",
              "  'fbnetc_100',\n",
              "  'fbnetv3_b',\n",
              "  'fbnetv3_d',\n",
              "  'fbnetv3_g',\n",
              "  'gc_efficientnetv2_rw_t',\n",
              "  'gcresnet33ts',\n",
              "  'gcresnet50t',\n",
              "  'gcresnext26ts',\n",
              "  'gcresnext50ts',\n",
              "  'gernet_l',\n",
              "  'gernet_m',\n",
              "  'gernet_s',\n",
              "  'ghostnet_100',\n",
              "  'gluon_inception_v3',\n",
              "  'gluon_resnet18_v1b',\n",
              "  'gluon_resnet34_v1b',\n",
              "  'gluon_resnet50_v1b',\n",
              "  'gluon_resnet50_v1c',\n",
              "  'gluon_resnet50_v1d',\n",
              "  'gluon_resnet50_v1s',\n",
              "  'gluon_resnet101_v1b',\n",
              "  'gluon_resnet101_v1c',\n",
              "  'gluon_resnet101_v1d',\n",
              "  'gluon_resnet101_v1s',\n",
              "  'gluon_resnet152_v1b',\n",
              "  'gluon_resnet152_v1c',\n",
              "  'gluon_resnet152_v1d',\n",
              "  'gluon_resnet152_v1s',\n",
              "  'gluon_resnext50_32x4d',\n",
              "  'gluon_resnext101_32x4d',\n",
              "  'gluon_resnext101_64x4d',\n",
              "  'gluon_senet154',\n",
              "  'gluon_seresnext50_32x4d',\n",
              "  'gluon_seresnext101_32x4d',\n",
              "  'gluon_seresnext101_64x4d',\n",
              "  'gluon_xception65',\n",
              "  'gmixer_24_224',\n",
              "  'gmlp_s16_224',\n",
              "  'halo2botnet50ts_256',\n",
              "  'halonet26t',\n",
              "  'halonet50ts',\n",
              "  'haloregnetz_b',\n",
              "  'hardcorenas_a',\n",
              "  'hardcorenas_b',\n",
              "  'hardcorenas_c',\n",
              "  'hardcorenas_d',\n",
              "  'hardcorenas_e',\n",
              "  'hardcorenas_f',\n",
              "  'hrnet_w18',\n",
              "  'hrnet_w18_small',\n",
              "  'hrnet_w18_small_v2',\n",
              "  'hrnet_w30',\n",
              "  'hrnet_w32',\n",
              "  'hrnet_w40',\n",
              "  'hrnet_w44',\n",
              "  'hrnet_w48',\n",
              "  'hrnet_w64',\n",
              "  'ig_resnext101_32x8d',\n",
              "  'ig_resnext101_32x16d',\n",
              "  'ig_resnext101_32x32d',\n",
              "  'ig_resnext101_32x48d',\n",
              "  'inception_resnet_v2',\n",
              "  'inception_v3',\n",
              "  'inception_v4',\n",
              "  'jx_nest_base',\n",
              "  'jx_nest_small',\n",
              "  'jx_nest_tiny',\n",
              "  'lambda_resnet26rpt_256',\n",
              "  'lambda_resnet26t',\n",
              "  'lambda_resnet50ts',\n",
              "  'lamhalobotnet50ts_256',\n",
              "  'lcnet_050',\n",
              "  'lcnet_075',\n",
              "  'lcnet_100',\n",
              "  'legacy_senet154',\n",
              "  'legacy_seresnet18',\n",
              "  'legacy_seresnet34',\n",
              "  'legacy_seresnet50',\n",
              "  'legacy_seresnet101',\n",
              "  'legacy_seresnet152',\n",
              "  'legacy_seresnext26_32x4d',\n",
              "  'legacy_seresnext50_32x4d',\n",
              "  'legacy_seresnext101_32x4d',\n",
              "  'levit_128',\n",
              "  'levit_128s',\n",
              "  'levit_192',\n",
              "  'levit_256',\n",
              "  'levit_384',\n",
              "  'mixer_b16_224',\n",
              "  'mixer_b16_224_in21k',\n",
              "  'mixer_b16_224_miil',\n",
              "  'mixer_b16_224_miil_in21k',\n",
              "  'mixer_l16_224',\n",
              "  'mixer_l16_224_in21k',\n",
              "  'mixnet_l',\n",
              "  'mixnet_m',\n",
              "  'mixnet_s',\n",
              "  'mixnet_xl',\n",
              "  'mnasnet_100',\n",
              "  'mnasnet_small',\n",
              "  'mobilenetv2_050',\n",
              "  'mobilenetv2_100',\n",
              "  'mobilenetv2_110d',\n",
              "  'mobilenetv2_120d',\n",
              "  'mobilenetv2_140',\n",
              "  'mobilenetv3_large_100',\n",
              "  'mobilenetv3_large_100_miil',\n",
              "  'mobilenetv3_large_100_miil_in21k',\n",
              "  'mobilenetv3_rw',\n",
              "  'nasnetalarge',\n",
              "  'nf_regnet_b1',\n",
              "  'nf_resnet50',\n",
              "  'nfnet_l0',\n",
              "  'pit_b_224',\n",
              "  'pit_b_distilled_224',\n",
              "  'pit_s_224',\n",
              "  'pit_s_distilled_224',\n",
              "  'pit_ti_224',\n",
              "  'pit_ti_distilled_224',\n",
              "  'pit_xs_224',\n",
              "  'pit_xs_distilled_224',\n",
              "  'pnasnet5large',\n",
              "  'regnetx_002',\n",
              "  'regnetx_004',\n",
              "  'regnetx_006',\n",
              "  'regnetx_008',\n",
              "  'regnetx_016',\n",
              "  'regnetx_032',\n",
              "  'regnetx_040',\n",
              "  'regnetx_064',\n",
              "  'regnetx_080',\n",
              "  'regnetx_120',\n",
              "  'regnetx_160',\n",
              "  'regnetx_320',\n",
              "  'regnety_002',\n",
              "  'regnety_004',\n",
              "  'regnety_006',\n",
              "  'regnety_008',\n",
              "  'regnety_016',\n",
              "  'regnety_032',\n",
              "  'regnety_040',\n",
              "  'regnety_064',\n",
              "  'regnety_080',\n",
              "  'regnety_120',\n",
              "  'regnety_160',\n",
              "  'regnety_320',\n",
              "  'regnetz_b16',\n",
              "  'regnetz_c16',\n",
              "  'regnetz_d8',\n",
              "  'regnetz_d32',\n",
              "  'regnetz_e8',\n",
              "  'repvgg_a2',\n",
              "  'repvgg_b0',\n",
              "  'repvgg_b1',\n",
              "  'repvgg_b1g4',\n",
              "  'repvgg_b2',\n",
              "  'repvgg_b2g4',\n",
              "  'repvgg_b3',\n",
              "  'repvgg_b3g4',\n",
              "  'res2net50_14w_8s',\n",
              "  'res2net50_26w_4s',\n",
              "  'res2net50_26w_6s',\n",
              "  'res2net50_26w_8s',\n",
              "  'res2net50_48w_2s',\n",
              "  'res2net101_26w_4s',\n",
              "  'res2next50',\n",
              "  'resmlp_12_224',\n",
              "  'resmlp_12_224_dino',\n",
              "  'resmlp_12_distilled_224',\n",
              "  'resmlp_24_224',\n",
              "  'resmlp_24_224_dino',\n",
              "  'resmlp_24_distilled_224',\n",
              "  'resmlp_36_224',\n",
              "  'resmlp_36_distilled_224',\n",
              "  'resmlp_big_24_224',\n",
              "  'resmlp_big_24_224_in22ft1k',\n",
              "  'resmlp_big_24_distilled_224',\n",
              "  'resnest14d',\n",
              "  'resnest26d',\n",
              "  'resnest50d',\n",
              "  'resnest50d_1s4x24d',\n",
              "  'resnest50d_4s2x40d',\n",
              "  'resnest101e',\n",
              "  'resnest200e',\n",
              "  'resnest269e',\n",
              "  'resnet18',\n",
              "  'resnet18d',\n",
              "  'resnet26',\n",
              "  'resnet26d',\n",
              "  'resnet26t',\n",
              "  'resnet32ts',\n",
              "  'resnet33ts',\n",
              "  'resnet34',\n",
              "  'resnet34d',\n",
              "  'resnet50',\n",
              "  'resnet50_gn',\n",
              "  'resnet50d',\n",
              "  'resnet51q',\n",
              "  'resnet61q',\n",
              "  'resnet101',\n",
              "  'resnet101d',\n",
              "  'resnet152',\n",
              "  'resnet152d',\n",
              "  'resnet200d',\n",
              "  'resnetblur50',\n",
              "  'resnetrs50',\n",
              "  'resnetrs101',\n",
              "  'resnetrs152',\n",
              "  'resnetrs200',\n",
              "  'resnetrs270',\n",
              "  'resnetrs350',\n",
              "  'resnetrs420',\n",
              "  'resnetv2_50',\n",
              "  'resnetv2_50x1_bit_distilled',\n",
              "  'resnetv2_50x1_bitm',\n",
              "  'resnetv2_50x1_bitm_in21k',\n",
              "  'resnetv2_50x3_bitm',\n",
              "  'resnetv2_50x3_bitm_in21k',\n",
              "  'resnetv2_101',\n",
              "  'resnetv2_101x1_bitm',\n",
              "  'resnetv2_101x1_bitm_in21k',\n",
              "  'resnetv2_101x3_bitm',\n",
              "  'resnetv2_101x3_bitm_in21k',\n",
              "  'resnetv2_152x2_bit_teacher',\n",
              "  'resnetv2_152x2_bit_teacher_384',\n",
              "  'resnetv2_152x2_bitm',\n",
              "  'resnetv2_152x2_bitm_in21k',\n",
              "  'resnetv2_152x4_bitm',\n",
              "  'resnetv2_152x4_bitm_in21k',\n",
              "  'resnext26ts',\n",
              "  'resnext50_32x4d',\n",
              "  'resnext50d_32x4d',\n",
              "  'resnext101_32x8d',\n",
              "  'rexnet_100',\n",
              "  'rexnet_130',\n",
              "  'rexnet_150',\n",
              "  'rexnet_200',\n",
              "  'sebotnet33ts_256',\n",
              "  'sehalonet33ts',\n",
              "  'selecsls42b',\n",
              "  'selecsls60',\n",
              "  'selecsls60b',\n",
              "  'semnasnet_075',\n",
              "  'semnasnet_100',\n",
              "  'seresnet33ts',\n",
              "  'seresnet50',\n",
              "  'seresnet152d',\n",
              "  'seresnext26d_32x4d',\n",
              "  'seresnext26t_32x4d',\n",
              "  'seresnext26ts',\n",
              "  'seresnext50_32x4d',\n",
              "  'skresnet18',\n",
              "  'skresnet34',\n",
              "  'skresnext50_32x4d',\n",
              "  'spnasnet_100',\n",
              "  'ssl_resnet18',\n",
              "  'ssl_resnet50',\n",
              "  'ssl_resnext50_32x4d',\n",
              "  'ssl_resnext101_32x4d',\n",
              "  'ssl_resnext101_32x8d',\n",
              "  'ssl_resnext101_32x16d',\n",
              "  'swin_base_patch4_window7_224',\n",
              "  'swin_base_patch4_window7_224_in22k',\n",
              "  'swin_base_patch4_window12_384',\n",
              "  'swin_base_patch4_window12_384_in22k',\n",
              "  'swin_large_patch4_window7_224',\n",
              "  'swin_large_patch4_window7_224_in22k',\n",
              "  'swin_large_patch4_window12_384',\n",
              "  'swin_large_patch4_window12_384_in22k',\n",
              "  'swin_small_patch4_window7_224',\n",
              "  'swin_tiny_patch4_window7_224',\n",
              "  'swsl_resnet18',\n",
              "  'swsl_resnet50',\n",
              "  'swsl_resnext50_32x4d',\n",
              "  'swsl_resnext101_32x4d',\n",
              "  'swsl_resnext101_32x8d',\n",
              "  'swsl_resnext101_32x16d',\n",
              "  'tf_efficientnet_b0',\n",
              "  'tf_efficientnet_b0_ap',\n",
              "  'tf_efficientnet_b0_ns',\n",
              "  'tf_efficientnet_b1',\n",
              "  'tf_efficientnet_b1_ap',\n",
              "  'tf_efficientnet_b1_ns',\n",
              "  'tf_efficientnet_b2',\n",
              "  'tf_efficientnet_b2_ap',\n",
              "  'tf_efficientnet_b2_ns',\n",
              "  'tf_efficientnet_b3',\n",
              "  'tf_efficientnet_b3_ap',\n",
              "  'tf_efficientnet_b3_ns',\n",
              "  'tf_efficientnet_b4',\n",
              "  'tf_efficientnet_b4_ap',\n",
              "  'tf_efficientnet_b4_ns',\n",
              "  'tf_efficientnet_b5',\n",
              "  'tf_efficientnet_b5_ap',\n",
              "  'tf_efficientnet_b5_ns',\n",
              "  'tf_efficientnet_b6',\n",
              "  'tf_efficientnet_b6_ap',\n",
              "  'tf_efficientnet_b6_ns',\n",
              "  'tf_efficientnet_b7',\n",
              "  'tf_efficientnet_b7_ap',\n",
              "  'tf_efficientnet_b7_ns',\n",
              "  'tf_efficientnet_b8',\n",
              "  'tf_efficientnet_b8_ap',\n",
              "  'tf_efficientnet_cc_b0_4e',\n",
              "  'tf_efficientnet_cc_b0_8e',\n",
              "  'tf_efficientnet_cc_b1_8e',\n",
              "  'tf_efficientnet_el',\n",
              "  'tf_efficientnet_em',\n",
              "  'tf_efficientnet_es',\n",
              "  'tf_efficientnet_l2_ns',\n",
              "  'tf_efficientnet_l2_ns_475',\n",
              "  'tf_efficientnet_lite0',\n",
              "  'tf_efficientnet_lite1',\n",
              "  'tf_efficientnet_lite2',\n",
              "  'tf_efficientnet_lite3',\n",
              "  'tf_efficientnet_lite4',\n",
              "  'tf_efficientnetv2_b0',\n",
              "  'tf_efficientnetv2_b1',\n",
              "  'tf_efficientnetv2_b2',\n",
              "  'tf_efficientnetv2_b3',\n",
              "  'tf_efficientnetv2_l',\n",
              "  'tf_efficientnetv2_l_in21ft1k',\n",
              "  'tf_efficientnetv2_l_in21k',\n",
              "  'tf_efficientnetv2_m',\n",
              "  'tf_efficientnetv2_m_in21ft1k',\n",
              "  'tf_efficientnetv2_m_in21k',\n",
              "  'tf_efficientnetv2_s',\n",
              "  'tf_efficientnetv2_s_in21ft1k',\n",
              "  'tf_efficientnetv2_s_in21k',\n",
              "  'tf_efficientnetv2_xl_in21ft1k',\n",
              "  'tf_efficientnetv2_xl_in21k',\n",
              "  'tf_inception_v3',\n",
              "  'tf_mixnet_l',\n",
              "  'tf_mixnet_m',\n",
              "  'tf_mixnet_s',\n",
              "  'tf_mobilenetv3_large_075',\n",
              "  'tf_mobilenetv3_large_100',\n",
              "  'tf_mobilenetv3_large_minimal_100',\n",
              "  'tf_mobilenetv3_small_075',\n",
              "  'tf_mobilenetv3_small_100',\n",
              "  'tf_mobilenetv3_small_minimal_100',\n",
              "  'tinynet_a',\n",
              "  'tinynet_b',\n",
              "  'tinynet_c',\n",
              "  'tinynet_d',\n",
              "  'tinynet_e',\n",
              "  'tnt_s_patch16_224',\n",
              "  'tresnet_l',\n",
              "  'tresnet_l_448',\n",
              "  'tresnet_m',\n",
              "  'tresnet_m_448',\n",
              "  'tresnet_m_miil_in21k',\n",
              "  'tresnet_xl',\n",
              "  'tresnet_xl_448',\n",
              "  'tv_densenet121',\n",
              "  'tv_resnet34',\n",
              "  'tv_resnet50',\n",
              "  'tv_resnet101',\n",
              "  'tv_resnet152',\n",
              "  'tv_resnext50_32x4d',\n",
              "  'twins_pcpvt_base',\n",
              "  'twins_pcpvt_large',\n",
              "  'twins_pcpvt_small',\n",
              "  'twins_svt_base',\n",
              "  'twins_svt_large',\n",
              "  'twins_svt_small',\n",
              "  'vgg11',\n",
              "  'vgg11_bn',\n",
              "  'vgg13',\n",
              "  'vgg13_bn',\n",
              "  'vgg16',\n",
              "  'vgg16_bn',\n",
              "  'vgg19',\n",
              "  'vgg19_bn',\n",
              "  'visformer_small',\n",
              "  'vit_base_patch8_224',\n",
              "  'vit_base_patch8_224_in21k',\n",
              "  'vit_base_patch16_224',\n",
              "  'vit_base_patch16_224_in21k',\n",
              "  'vit_base_patch16_224_miil',\n",
              "  'vit_base_patch16_224_miil_in21k',\n",
              "  'vit_base_patch16_384',\n",
              "  'vit_base_patch16_sam_224',\n",
              "  'vit_base_patch32_224',\n",
              "  'vit_base_patch32_224_in21k',\n",
              "  'vit_base_patch32_384',\n",
              "  'vit_base_patch32_sam_224',\n",
              "  'vit_base_r50_s16_224_in21k',\n",
              "  'vit_base_r50_s16_384',\n",
              "  'vit_huge_patch14_224_in21k',\n",
              "  'vit_large_patch16_224',\n",
              "  'vit_large_patch16_224_in21k',\n",
              "  'vit_large_patch16_384',\n",
              "  'vit_large_patch32_224_in21k',\n",
              "  'vit_large_patch32_384',\n",
              "  'vit_large_r50_s32_224',\n",
              "  'vit_large_r50_s32_224_in21k',\n",
              "  'vit_large_r50_s32_384',\n",
              "  'vit_small_patch16_224',\n",
              "  'vit_small_patch16_224_in21k',\n",
              "  'vit_small_patch16_384',\n",
              "  'vit_small_patch32_224',\n",
              "  'vit_small_patch32_224_in21k',\n",
              "  'vit_small_patch32_384',\n",
              "  'vit_small_r26_s32_224',\n",
              "  'vit_small_r26_s32_224_in21k',\n",
              "  'vit_small_r26_s32_384',\n",
              "  'vit_tiny_patch16_224',\n",
              "  'vit_tiny_patch16_224_in21k',\n",
              "  'vit_tiny_patch16_384',\n",
              "  'vit_tiny_r_s16_p8_224',\n",
              "  'vit_tiny_r_s16_p8_224_in21k',\n",
              "  'vit_tiny_r_s16_p8_384',\n",
              "  'wide_resnet50_2',\n",
              "  'wide_resnet101_2',\n",
              "  'xception',\n",
              "  'xception41',\n",
              "  'xception65',\n",
              "  'xception71',\n",
              "  'xcit_large_24_p8_224',\n",
              "  'xcit_large_24_p8_224_dist',\n",
              "  'xcit_large_24_p8_384_dist',\n",
              "  'xcit_large_24_p16_224',\n",
              "  'xcit_large_24_p16_224_dist',\n",
              "  'xcit_large_24_p16_384_dist',\n",
              "  'xcit_medium_24_p8_224',\n",
              "  'xcit_medium_24_p8_224_dist',\n",
              "  'xcit_medium_24_p8_384_dist',\n",
              "  'xcit_medium_24_p16_224',\n",
              "  'xcit_medium_24_p16_224_dist',\n",
              "  'xcit_medium_24_p16_384_dist',\n",
              "  'xcit_nano_12_p8_224',\n",
              "  'xcit_nano_12_p8_224_dist',\n",
              "  'xcit_nano_12_p8_384_dist',\n",
              "  'xcit_nano_12_p16_224',\n",
              "  'xcit_nano_12_p16_224_dist',\n",
              "  'xcit_nano_12_p16_384_dist',\n",
              "  'xcit_small_12_p8_224',\n",
              "  'xcit_small_12_p8_224_dist',\n",
              "  'xcit_small_12_p8_384_dist',\n",
              "  'xcit_small_12_p16_224',\n",
              "  'xcit_small_12_p16_224_dist',\n",
              "  'xcit_small_12_p16_384_dist',\n",
              "  'xcit_small_24_p8_224',\n",
              "  'xcit_small_24_p8_224_dist',\n",
              "  'xcit_small_24_p8_384_dist',\n",
              "  'xcit_small_24_p16_224',\n",
              "  'xcit_small_24_p16_224_dist',\n",
              "  'xcit_small_24_p16_384_dist',\n",
              "  'xcit_tiny_12_p8_224',\n",
              "  'xcit_tiny_12_p8_224_dist',\n",
              "  'xcit_tiny_12_p8_384_dist',\n",
              "  'xcit_tiny_12_p16_224',\n",
              "  'xcit_tiny_12_p16_224_dist',\n",
              "  'xcit_tiny_12_p16_384_dist',\n",
              "  'xcit_tiny_24_p8_224',\n",
              "  'xcit_tiny_24_p8_224_dist',\n",
              "  'xcit_tiny_24_p8_384_dist',\n",
              "  'xcit_tiny_24_p16_224',\n",
              "  'xcit_tiny_24_p16_224_dist',\n",
              "  'xcit_tiny_24_p16_384_dist'])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'convnext_base_384_in22ft1k'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"device = \", device)\n",
        "model = create_model(model_name, pretrained=True).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqpAZ-TdFuNs",
        "outputId": "15f0d6b8-b836-49e5-fe24-f038c25e13d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device =  cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_384.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base_22k_1k_384.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYtx4ylJOjzz",
        "outputId": "5cd13d8c-cbac-4da7-b38c-3c89800e5f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNeXt(\n",
            "  (stem): Sequential(\n",
            "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
            "    (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (stages): Sequential(\n",
            "    (0): ConvNeXtStage(\n",
            "      (downsample): Identity()\n",
            "      (blocks): Sequential(\n",
            "        (0): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
            "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (1): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
            "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (2): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
            "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): ConvNeXtStage(\n",
            "      (downsample): Sequential(\n",
            "        (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
            "        (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      )\n",
            "      (blocks): Sequential(\n",
            "        (0): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
            "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (1): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
            "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (2): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
            "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): ConvNeXtStage(\n",
            "      (downsample): Sequential(\n",
            "        (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
            "      )\n",
            "      (blocks): Sequential(\n",
            "        (0): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (1): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (2): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (3): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (4): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (5): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (6): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (7): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (8): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (9): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (10): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (11): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (12): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (13): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (14): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (15): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (16): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (17): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (18): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (19): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (20): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (21): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (22): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (23): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (24): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (25): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (26): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
            "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): ConvNeXtStage(\n",
            "      (downsample): Sequential(\n",
            "        (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
            "      )\n",
            "      (blocks): Sequential(\n",
            "        (0): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
            "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (1): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
            "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "        (2): ConvNeXtBlock(\n",
            "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
            "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm_pre): Identity()\n",
            "  (head): Sequential(\n",
            "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
            "    (norm): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myConvNextBase384(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(myConvNextBase384, self).__init__()\n",
        "    self.model_ft = create_model('convnext_base_384_in22ft1k', pretrained=True)\n",
        "    \n",
        "    num_ftrs = self.model_ft.head.fc.in_features\n",
        "    self.model_ft.head.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.model_ft(x)\n",
        "      return out"
      ],
      "metadata": {
        "id": "PBttn3j7Oqau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convnext_base_384 = myConvNextBase384(3)"
      ],
      "metadata": {
        "id": "3ultYnF5O_-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2bea578-f951-46e4-8ab5-fc5a9eebb48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_384.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base_22k_1k_384.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "train_loader, val_loader, test_loader = make_data_loader_with_aug(batch_size=8, size=384)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = optim.Adam(convnext_base_384.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "MODEL_DIR = '/content/gdrive/MyDrive/ML/MRDC-competition/models'\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.00001, path=os.path.join(MODEL_DIR, \"convnext_base_384_aug_checkpoint.pt\"))"
      ],
      "metadata": {
        "id": "g1zETaWoPG16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_rgb['Label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUnvfAMSz_78",
        "outputId": "7e9b2570-36b2-430a-9354-1a4d47f2fd01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "blast      1494\n",
              "brown       766\n",
              "healthy     410\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convnext_base_384, val_label, val_pred = train_model(device, convnext_base_384, train_loader, val_loader, criterion, optimizer, 30, early_stopping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyyZWjwayjuW",
        "outputId": "a49ab752-28aa-42dd-e0d9-d6a6e0bd5bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "train Loss: 0.5779 Acc: 0.7591\n",
            "val Loss: 0.4516 Acc: 0.8287\n",
            "Validation loss decreased (inf --> 0.451625).  Saving model ...\n",
            "Epoch 2/30\n",
            "----------\n",
            "train Loss: 0.1935 Acc: 0.9354\n",
            "val Loss: 0.1564 Acc: 0.9550\n",
            "Validation loss decreased (0.451625 --> 0.156423).  Saving model ...\n",
            "Epoch 3/30\n",
            "----------\n",
            "train Loss: 0.1154 Acc: 0.9587\n",
            "val Loss: 0.0771 Acc: 0.9722\n",
            "Validation loss decreased (0.156423 --> 0.077071).  Saving model ...\n",
            "Epoch 4/30\n",
            "----------\n",
            "train Loss: 0.0731 Acc: 0.9762\n",
            "val Loss: 0.1650 Acc: 0.9315\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 5/30\n",
            "----------\n",
            "train Loss: 0.0428 Acc: 0.9853\n",
            "val Loss: 0.0724 Acc: 0.9807\n",
            "Validation loss decreased (0.077071 --> 0.072402).  Saving model ...\n",
            "Epoch 6/30\n",
            "----------\n",
            "train Loss: 0.0886 Acc: 0.9689\n",
            "val Loss: 0.1455 Acc: 0.9572\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 7/30\n",
            "----------\n",
            "train Loss: 0.0445 Acc: 0.9842\n",
            "val Loss: 0.4734 Acc: 0.8844\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 8/30\n",
            "----------\n",
            "train Loss: 0.0480 Acc: 0.9847\n",
            "val Loss: 0.1094 Acc: 0.9700\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 9/30\n",
            "----------\n",
            "train Loss: 0.0486 Acc: 0.9839\n",
            "val Loss: 0.2090 Acc: 0.9315\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch 10/30\n",
            "----------\n",
            "train Loss: 0.0414 Acc: 0.9882\n",
            "val Loss: 0.1232 Acc: 0.9679\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['blast', 'brown', 'healthy']\n",
        "plot_confusion_matrix(confusion_matrix(val_label, val_pred), target_names=classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "js3Ksn7O1kfV",
        "outputId": "d600311a-dfe3-44ea-d271-cb1a9adfe2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-7a0f60b4bf57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'blast'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'brown'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'healthy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'val_label' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convnext_base_384.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"convnext_base_384_aug_checkpoint.pt\")))\n",
        "test_pred = test_model(device, convnext_base_384, test_loader)"
      ],
      "metadata": {
        "id": "4CVT13nq1qyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss.loc[:, ['blast', 'brown', 'healthy']] = test_pred\n",
        "ss.to_csv(\"result_convnext_base384_aug2.csv\", index=False)\n",
        "ss.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VZwKReu31uLa",
        "outputId": "547a03d8-0d8a-4003-d685-be3ffb28637b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Image_id     blast     brown   healthy\n",
              "0  id_00vl5wvxq3.jpg  0.999346  0.000636  0.000018\n",
              "1  id_01hu05mtch.jpg  0.124495  0.875465  0.000039\n",
              "2  id_030ln10ewn.jpg  0.887034  0.112725  0.000242\n",
              "3  id_03z57m8xht.jpg  0.999986  0.000011  0.000003\n",
              "4  id_04ngep1w4b.jpg  0.995631  0.004227  0.000141"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-81afc765-9909-4d55-af13-a8ca9877d9c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_id</th>\n",
              "      <th>blast</th>\n",
              "      <th>brown</th>\n",
              "      <th>healthy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_00vl5wvxq3.jpg</td>\n",
              "      <td>0.999346</td>\n",
              "      <td>0.000636</td>\n",
              "      <td>0.000018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_01hu05mtch.jpg</td>\n",
              "      <td>0.124495</td>\n",
              "      <td>0.875465</td>\n",
              "      <td>0.000039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_030ln10ewn.jpg</td>\n",
              "      <td>0.887034</td>\n",
              "      <td>0.112725</td>\n",
              "      <td>0.000242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_03z57m8xht.jpg</td>\n",
              "      <td>0.999986</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_04ngep1w4b.jpg</td>\n",
              "      <td>0.995631</td>\n",
              "      <td>0.004227</td>\n",
              "      <td>0.000141</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81afc765-9909-4d55-af13-a8ca9877d9c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-81afc765-9909-4d55-af13-a8ca9877d9c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-81afc765-9909-4d55-af13-a8ca9877d9c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "score : 0.19 (SOTA) 근데 너무 오래 걸림..ㅎ"
      ],
      "metadata": {
        "id": "aCth02v7LayI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Target 별 Loss 구하는 코드"
      ],
      "metadata": {
        "id": "XvQ2GC6MZqOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def val_model(device, model, val_loader):\n",
        "    test_pred = []\n",
        "\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    val_target = []\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for features, target in val_loader:\n",
        "            features = features.to(device)\n",
        "            val_target += target.tolist()\n",
        "\n",
        "            outputs = model(features.to(torch.float))\n",
        "            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "            test_pred.append(probabilities.tolist())\n",
        "\n",
        "    return test_pred, val_target"
      ],
      "metadata": {
        "id": "8CTcpOh6Pu5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(val_loader.dataset, batch_size=1, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "D1op8XfTVMIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred, val_target = val_model(device, convnext_base_384, val_loader)"
      ],
      "metadata": {
        "id": "FeTfzoJ5LeJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def log_loss_by_label(val_target, val_pred):\n",
        "  val_0 = []\n",
        "  val_1 = []\n",
        "  val_2 = []\n",
        "\n",
        "  for i, target in enumerate(val_target):\n",
        "    if target == 0:\n",
        "      val_0.append(val_pred[i])\n",
        "    elif target == 1:\n",
        "      val_1.append(val_pred[i])\n",
        "    elif target == 2:\n",
        "      val_2.append(val_pred[i])\n",
        "  print(\"blast :\")\n",
        "  print(log_loss([[1, 0, 0] for _ in range(len(val_0))], val_0))\n",
        "  print(\"brown :\")\n",
        "  print(log_loss([[0, 1, 0] for _ in range(len(val_1))], val_1))\n",
        "  print(\"healthy :\")\n",
        "  print(log_loss([[0, 0, 1] for _ in range(len(val_2))], val_2))"
      ],
      "metadata": {
        "id": "coyy0EDsQHS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_loss_by_label(val_target, val_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP2DFy18UrXw",
        "outputId": "87932ac0-db00-436e-c321-7d708fba56b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blast :\n",
            "0.1029236394833858\n",
            "brown :\n",
            "0.18469007093496678\n",
            "healthy :\n",
            "0.036418428098195094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNext에서 Target 별 log loss 구하기"
      ],
      "metadata": {
        "id": "vf6nrpGyY-Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myResNeXt50(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(myResNeXt50, self).__init__()\n",
        "    self.model_ft = models.resnext50_32x4d(weights=models.ResNeXt50_32X4D_Weights.DEFAULT)\n",
        "    \n",
        "    num_ftrs = self.model_ft.fc.in_features\n",
        "    self.model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.model_ft(x)\n",
        "      return out"
      ],
      "metadata": {
        "id": "-y0PFvTvY_Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnext50_tf = myResNeXt50(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88,
          "referenced_widgets": [
            "f607a97a9f61430da604dd03bab5444f",
            "5662be5e09f44af98a1977260635569d",
            "31fd70e65e4a49038fd1d94c654f6306",
            "bdd9d7d7dca34ead8b02ffab68628ebc",
            "9dd66476cb804f4c959243e61b144c03",
            "1b26a6aefc7c4c0eb84c2f9359f067c2",
            "a4e4b5c65beb4301864130376bae0a44",
            "8407415369aa47daae1a5e1cfdb4ca47",
            "e73822bf9f2a4ce5930f60c53da77339",
            "bbd76149d0874015b13fa0fe4b09e614",
            "6ee5e0a848a047d7a0328620a92b10bf"
          ]
        },
        "id": "CAUUvWcHZCX7",
        "outputId": "7e9080ff-8bd2-4289-9515-33bdb7d1b565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-1a0047aa.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-1a0047aa.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/95.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f607a97a9f61430da604dd03bab5444f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnext50_tf.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"resnext50-aug_checkpoint.pt\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErbXrgQlZDUf",
        "outputId": "000ec77d-7bee-41d9-c474-96897c82bdec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred, val_target = val_model(device, resnext50_tf, val_loader)"
      ],
      "metadata": {
        "id": "qj_DeitFZMFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_loss_by_label(val_target, val_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crCaTQg4ZSPS",
        "outputId": "788884c3-863a-466a-db1f-9aeae76474e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blast :\n",
            "0.11601280693742066\n",
            "brown :\n",
            "0.3763652798551647\n",
            "healthy :\n",
            "0.4659342289541602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SEResNext50_32x4d"
      ],
      "metadata": {
        "id": "N8c9FERv1y9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = create_model('seresnext50_32x4d', pretrained=True)\n",
        "print(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agVR0-c812NB",
        "outputId": "fad3b106-f395-4900-aa8d-baacd1736a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext50_32x4d_racm-a304a460.pth\" to /root/.cache/torch/hub/checkpoints/seresnext50_32x4d_racm-a304a460.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (act1): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SEModule(\n",
            "        (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (bn): Identity()\n",
            "        (act): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (gate): Sigmoid()\n",
            "      )\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class mySEResNext50(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(mySEResNext50, self).__init__()\n",
        "    self.model_ft = create_model('seresnext50_32x4d', pretrained=True)\n",
        "    \n",
        "    num_ftrs = self.model_ft.fc.in_features\n",
        "    self.model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.model_ft(x)\n",
        "      return out"
      ],
      "metadata": {
        "id": "NG4tC34f2JV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seresnext50_tf = mySEResNext50(3)"
      ],
      "metadata": {
        "id": "boZd_E982TbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "train_loader, val_loader, test_loader = make_data_loader_with_aug(batch_size=32, size=224)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = optim.Adam(seresnext50_tf.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "MODEL_DIR = '/content/gdrive/MyDrive/ML/MRDC-competition/models'\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.00001, path=os.path.join(MODEL_DIR, \"seresnext50_aug_checkpoint.pt\"))"
      ],
      "metadata": {
        "id": "rxd91B7gFNcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seresnext50_tf, val_label, val_pred = train_model(device, seresnext50_tf, train_loader, val_loader, criterion, optimizer, 30, early_stopping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erdXcS3OFcy_",
        "outputId": "bdca8f46-8030-4646-8e5e-eebbe01ca64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "train Loss: 0.4925 Acc: 0.8087\n",
            "val Loss: 0.1685 Acc: 0.9379\n",
            "Validation loss decreased (inf --> 0.168491).  Saving model ...\n",
            "Epoch 2/30\n",
            "----------\n",
            "train Loss: 0.1178 Acc: 0.9614\n",
            "val Loss: 0.1589 Acc: 0.9443\n",
            "Validation loss decreased (0.168491 --> 0.158917).  Saving model ...\n",
            "Epoch 3/30\n",
            "----------\n",
            "train Loss: 0.0628 Acc: 0.9842\n",
            "val Loss: 0.1029 Acc: 0.9625\n",
            "Validation loss decreased (0.158917 --> 0.102941).  Saving model ...\n",
            "Epoch 4/30\n",
            "----------\n",
            "train Loss: 0.0316 Acc: 0.9909\n",
            "val Loss: 0.0998 Acc: 0.9647\n",
            "Validation loss decreased (0.102941 --> 0.099754).  Saving model ...\n",
            "Epoch 5/30\n",
            "----------\n",
            "train Loss: 0.0399 Acc: 0.9858\n",
            "val Loss: 0.1079 Acc: 0.9668\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 6/30\n",
            "----------\n",
            "train Loss: 0.0487 Acc: 0.9837\n",
            "val Loss: 0.1585 Acc: 0.9518\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 7/30\n",
            "----------\n",
            "train Loss: 0.0420 Acc: 0.9855\n",
            "val Loss: 0.1078 Acc: 0.9668\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 8/30\n",
            "----------\n",
            "train Loss: 0.0203 Acc: 0.9944\n",
            "val Loss: 0.0939 Acc: 0.9668\n",
            "Validation loss decreased (0.099754 --> 0.093854).  Saving model ...\n",
            "Epoch 9/30\n",
            "----------\n",
            "train Loss: 0.0241 Acc: 0.9941\n",
            "val Loss: 0.1507 Acc: 0.9593\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 10/30\n",
            "----------\n",
            "train Loss: 0.0215 Acc: 0.9925\n",
            "val Loss: 0.0893 Acc: 0.9722\n",
            "Validation loss decreased (0.093854 --> 0.089270).  Saving model ...\n",
            "Epoch 11/30\n",
            "----------\n",
            "train Loss: 0.0333 Acc: 0.9914\n",
            "val Loss: 0.0932 Acc: 0.9711\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 12/30\n",
            "----------\n",
            "train Loss: 0.0169 Acc: 0.9944\n",
            "val Loss: 0.0993 Acc: 0.9690\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 13/30\n",
            "----------\n",
            "train Loss: 0.0118 Acc: 0.9973\n",
            "val Loss: 0.1424 Acc: 0.9679\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 14/30\n",
            "----------\n",
            "train Loss: 0.0248 Acc: 0.9925\n",
            "val Loss: 0.1579 Acc: 0.9550\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch 15/30\n",
            "----------\n",
            "train Loss: 0.0281 Acc: 0.9909\n",
            "val Loss: 0.1601 Acc: 0.9700\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['blast', 'brown', 'healthy']\n",
        "plot_confusion_matrix(confusion_matrix(val_label, val_pred), target_names=classes)"
      ],
      "metadata": {
        "id": "Wt9JZg7qFoUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = test_model(device, seresnext50_tf, test_loader)"
      ],
      "metadata": {
        "id": "xDg4DqDLFota"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss.loc[:, ['blast', 'brown', 'healthy']] = test_pred\n",
        "ss.to_csv(\"result_seresnext_aug.csv\", index=False)\n",
        "ss.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7Y5Qtc6XFsma",
        "outputId": "0b62f6d6-51f1-4b60-8286-921763be1d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Image_id     blast         brown       healthy\n",
              "0  id_00vl5wvxq3.jpg  0.999999  1.147116e-07  4.949978e-07\n",
              "1  id_01hu05mtch.jpg  0.000175  9.996787e-01  1.459547e-04\n",
              "2  id_030ln10ewn.jpg  0.990332  1.391934e-03  8.276543e-03\n",
              "3  id_03z57m8xht.jpg  1.000000  4.183470e-11  5.567445e-11\n",
              "4  id_04ngep1w4b.jpg  0.998910  1.761415e-05  1.072641e-03"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66b2f117-0cbd-4ec7-b694-4133be432ca6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_id</th>\n",
              "      <th>blast</th>\n",
              "      <th>brown</th>\n",
              "      <th>healthy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_00vl5wvxq3.jpg</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>1.147116e-07</td>\n",
              "      <td>4.949978e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_01hu05mtch.jpg</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>9.996787e-01</td>\n",
              "      <td>1.459547e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_030ln10ewn.jpg</td>\n",
              "      <td>0.990332</td>\n",
              "      <td>1.391934e-03</td>\n",
              "      <td>8.276543e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_03z57m8xht.jpg</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.183470e-11</td>\n",
              "      <td>5.567445e-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_04ngep1w4b.jpg</td>\n",
              "      <td>0.998910</td>\n",
              "      <td>1.761415e-05</td>\n",
              "      <td>1.072641e-03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66b2f117-0cbd-4ec7-b694-4133be432ca6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-66b2f117-0cbd-4ec7-b694-4133be432ca6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-66b2f117-0cbd-4ec7-b694-4133be432ca6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "score : 0.25"
      ],
      "metadata": {
        "id": "_oxPsQ4Lfigt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Swin tiny patch4 window7 224"
      ],
      "metadata": {
        "id": "Fa-FXscMHtFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = create_model('swin_tiny_patch4_window7_224', pretrained=True)\n",
        "print(m)"
      ],
      "metadata": {
        "id": "fBPz1aZNHyyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf6dacb-3bcd-4f36-9ffe-90b7fda2990a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Downloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth\" to /root/.cache/torch/hub/checkpoints/swin_tiny_patch4_window7_224.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SwinTransformer(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
            "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (layers): Sequential(\n",
            "    (0): BasicLayer(\n",
            "      dim=96, input_resolution=(56, 56), depth=2\n",
            "      (blocks): ModuleList(\n",
            "        (0): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        input_resolution=(56, 56), dim=96\n",
            "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
            "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicLayer(\n",
            "      dim=192, input_resolution=(28, 28), depth=2\n",
            "      (blocks): ModuleList(\n",
            "        (0): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        input_resolution=(28, 28), dim=192\n",
            "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
            "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (2): BasicLayer(\n",
            "      dim=384, input_resolution=(14, 14), depth=6\n",
            "      (blocks): ModuleList(\n",
            "        (0): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        input_resolution=(14, 14), dim=384\n",
            "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
            "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (3): BasicLayer(\n",
            "      dim=768, input_resolution=(7, 7), depth=2\n",
            "      (blocks): ModuleList(\n",
            "        (0): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
            "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class mySwinTinyP4W7_224(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(mySwinTinyP4W7_224, self).__init__()\n",
        "    self.model_ft = create_model('swin_tiny_patch4_window7_224', pretrained=True)\n",
        "    \n",
        "    num_ftrs = self.model_ft.head.in_features\n",
        "    self.model_ft.head = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.model_ft(x)\n",
        "      return out"
      ],
      "metadata": {
        "id": "7erj6-QrICUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swin_tiny_tf = mySwinTinyP4W7_224(3)"
      ],
      "metadata": {
        "id": "KDFlvJZmMhzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "train_loader, val_loader, test_loader = make_data_loader_with_aug(batch_size=128, size=224)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = optim.Adam(swin_tiny_tf.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "MODEL_DIR = '/content/gdrive/MyDrive/ML/MRDC-competition/models'\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.00001, path=os.path.join(MODEL_DIR, \"swin_tiny_224_aug_checkpoint.pt\"))"
      ],
      "metadata": {
        "id": "4F0LSZonMw9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swin_tiny_tf, val_label, val_pred = train_model(device, swin_tiny_tf, train_loader, val_loader, criterion, optimizer, 30, early_stopping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOI3tsi1M3qA",
        "outputId": "d8ea4474-6d0a-467b-f2c6-b6c708a6f5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "train Loss: 0.7604 Acc: 0.6618\n",
            "val Loss: 0.5450 Acc: 0.7976\n",
            "Validation loss decreased (inf --> 0.545021).  Saving model ...\n",
            "Epoch 2/30\n",
            "----------\n",
            "train Loss: 0.3130 Acc: 0.8864\n",
            "val Loss: 0.2668 Acc: 0.9079\n",
            "Validation loss decreased (0.545021 --> 0.266824).  Saving model ...\n",
            "Epoch 3/30\n",
            "----------\n",
            "train Loss: 0.1191 Acc: 0.9571\n",
            "val Loss: 0.1960 Acc: 0.9261\n",
            "Validation loss decreased (0.266824 --> 0.195971).  Saving model ...\n",
            "Epoch 4/30\n",
            "----------\n",
            "train Loss: 0.0886 Acc: 0.9692\n",
            "val Loss: 0.2916 Acc: 0.9240\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 5/30\n",
            "----------\n",
            "train Loss: 0.0795 Acc: 0.9713\n",
            "val Loss: 0.6265 Acc: 0.8415\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 6/30\n",
            "----------\n",
            "train Loss: 0.1062 Acc: 0.9598\n",
            "val Loss: 0.1567 Acc: 0.9433\n",
            "Validation loss decreased (0.195971 --> 0.156654).  Saving model ...\n",
            "Epoch 7/30\n",
            "----------\n",
            "train Loss: 0.0903 Acc: 0.9713\n",
            "val Loss: 0.1468 Acc: 0.9550\n",
            "Validation loss decreased (0.156654 --> 0.146810).  Saving model ...\n",
            "Epoch 8/30\n",
            "----------\n",
            "train Loss: 0.0406 Acc: 0.9877\n",
            "val Loss: 0.0953 Acc: 0.9657\n",
            "Validation loss decreased (0.146810 --> 0.095250).  Saving model ...\n",
            "Epoch 9/30\n",
            "----------\n",
            "train Loss: 0.0221 Acc: 0.9925\n",
            "val Loss: 0.1236 Acc: 0.9700\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 10/30\n",
            "----------\n",
            "train Loss: 0.0267 Acc: 0.9909\n",
            "val Loss: 0.1160 Acc: 0.9679\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 11/30\n",
            "----------\n",
            "train Loss: 0.0860 Acc: 0.9695\n",
            "val Loss: 0.1081 Acc: 0.9625\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 12/30\n",
            "----------\n",
            "train Loss: 0.0124 Acc: 0.9976\n",
            "val Loss: 0.0752 Acc: 0.9786\n",
            "Validation loss decreased (0.095250 --> 0.075155).  Saving model ...\n",
            "Epoch 13/30\n",
            "----------\n",
            "train Loss: 0.0136 Acc: 0.9952\n",
            "val Loss: 0.2672 Acc: 0.9379\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 14/30\n",
            "----------\n",
            "train Loss: 0.0340 Acc: 0.9893\n",
            "val Loss: 0.2047 Acc: 0.9454\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 15/30\n",
            "----------\n",
            "train Loss: 0.0117 Acc: 0.9965\n",
            "val Loss: 0.1373 Acc: 0.9690\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 16/30\n",
            "----------\n",
            "train Loss: 0.0043 Acc: 0.9987\n",
            "val Loss: 0.1253 Acc: 0.9711\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch 17/30\n",
            "----------\n",
            "train Loss: 0.0018 Acc: 0.9995\n",
            "val Loss: 0.1336 Acc: 0.9679\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = test_model(device, swin_tiny_tf, test_loader)"
      ],
      "metadata": {
        "id": "zcwwrz2mTM8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss.loc[:, ['blast', 'brown', 'healthy']] = test_pred\n",
        "ss.to_csv(\"result_swin_tiny_224_aug.csv\", index=False)\n",
        "ss.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "78SL_2sQTbgR",
        "outputId": "5e9e5fd3-6cc4-49c0-8f5e-4aa24be806d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Image_id     blast         brown       healthy\n",
              "0  id_00vl5wvxq3.jpg  1.000000  8.112932e-09  1.180154e-09\n",
              "1  id_01hu05mtch.jpg  0.000475  9.995174e-01  7.347917e-06\n",
              "2  id_030ln10ewn.jpg  0.996637  2.477106e-03  8.859306e-04\n",
              "3  id_03z57m8xht.jpg  1.000000  1.022257e-10  1.329884e-11\n",
              "4  id_04ngep1w4b.jpg  0.970554  2.937046e-02  7.523574e-05"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92d5c6b1-b20c-4dec-ae15-e9cabda1ecd5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_id</th>\n",
              "      <th>blast</th>\n",
              "      <th>brown</th>\n",
              "      <th>healthy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_00vl5wvxq3.jpg</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.112932e-09</td>\n",
              "      <td>1.180154e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_01hu05mtch.jpg</td>\n",
              "      <td>0.000475</td>\n",
              "      <td>9.995174e-01</td>\n",
              "      <td>7.347917e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_030ln10ewn.jpg</td>\n",
              "      <td>0.996637</td>\n",
              "      <td>2.477106e-03</td>\n",
              "      <td>8.859306e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_03z57m8xht.jpg</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.022257e-10</td>\n",
              "      <td>1.329884e-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_04ngep1w4b.jpg</td>\n",
              "      <td>0.970554</td>\n",
              "      <td>2.937046e-02</td>\n",
              "      <td>7.523574e-05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92d5c6b1-b20c-4dec-ae15-e9cabda1ecd5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-92d5c6b1-b20c-4dec-ae15-e9cabda1ecd5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-92d5c6b1-b20c-4dec-ae15-e9cabda1ecd5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "score : 0.36\n",
        "transformer 기반 모델들은 성능이 다 안좋은듯.."
      ],
      "metadata": {
        "id": "4Tkhxnu1TuF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# resnext50d_32x4d (timm ver.)"
      ],
      "metadata": {
        "id": "c7Qp613KW0Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = create_model('resnext50d_32x4d', pretrained=True)\n",
        "print(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJYlnbdMW3D9",
        "outputId": "2d2ab487-9a4c-4f71-d971-c94da1bf097c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnext50d_32x4d-103e99f8.pth\" to /root/.cache/torch/hub/checkpoints/resnext50d_32x4d-103e99f8.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  )\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (act1): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Identity()\n",
            "        (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "        (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "        (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "        (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myResNext50d(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(myResNext50d, self).__init__()\n",
        "    self.model_ft = create_model('resnext50d_32x4d', pretrained=True)\n",
        "    \n",
        "    num_ftrs = self.model_ft.fc.in_features\n",
        "    self.model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.model_ft(x)\n",
        "      return out"
      ],
      "metadata": {
        "id": "W7YIQr8gW9Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnext50d_tf = myResNext50d(3)"
      ],
      "metadata": {
        "id": "AmdbatKjXLxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "train_loader, val_loader, test_loader = make_data_loader_with_aug(batch_size=32, size=224)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = optim.Adam(resnext50d_tf.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "MODEL_DIR = '/content/gdrive/MyDrive/ML/MRDC-competition/models'\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.00001, path=os.path.join(MODEL_DIR, \"resnext50d_aug_checkpoint.pt\"))"
      ],
      "metadata": {
        "id": "PXHH6_WZXRYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnext50d_tf, val_label, val_pred = train_model(device, resnext50d_tf, train_loader, val_loader, criterion, optimizer, 30, early_stopping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFMSTw6OXiQn",
        "outputId": "799e68cd-80e2-48ad-8780-f9181ac7150b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "train Loss: 0.3949 Acc: 0.8470\n",
            "val Loss: 0.2090 Acc: 0.9218\n",
            "Validation loss decreased (inf --> 0.209006).  Saving model ...\n",
            "Epoch 2/30\n",
            "----------\n",
            "train Loss: 0.0974 Acc: 0.9673\n",
            "val Loss: 0.1643 Acc: 0.9486\n",
            "Validation loss decreased (0.209006 --> 0.164260).  Saving model ...\n",
            "Epoch 3/30\n",
            "----------\n",
            "train Loss: 0.0859 Acc: 0.9692\n",
            "val Loss: 0.2007 Acc: 0.9411\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 4/30\n",
            "----------\n",
            "train Loss: 0.0854 Acc: 0.9721\n",
            "val Loss: 0.1512 Acc: 0.9582\n",
            "Validation loss decreased (0.164260 --> 0.151228).  Saving model ...\n",
            "Epoch 5/30\n",
            "----------\n",
            "train Loss: 0.0993 Acc: 0.9681\n",
            "val Loss: 0.1337 Acc: 0.9518\n",
            "Validation loss decreased (0.151228 --> 0.133662).  Saving model ...\n",
            "Epoch 6/30\n",
            "----------\n",
            "train Loss: 0.0683 Acc: 0.9788\n",
            "val Loss: 0.1016 Acc: 0.9657\n",
            "Validation loss decreased (0.133662 --> 0.101615).  Saving model ...\n",
            "Epoch 7/30\n",
            "----------\n",
            "train Loss: 0.0465 Acc: 0.9845\n",
            "val Loss: 0.0951 Acc: 0.9668\n",
            "Validation loss decreased (0.101615 --> 0.095101).  Saving model ...\n",
            "Epoch 8/30\n",
            "----------\n",
            "train Loss: 0.0378 Acc: 0.9893\n",
            "val Loss: 0.1579 Acc: 0.9582\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 9/30\n",
            "----------\n",
            "train Loss: 0.0763 Acc: 0.9753\n",
            "val Loss: 0.2753 Acc: 0.9390\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 10/30\n",
            "----------\n",
            "train Loss: 0.0781 Acc: 0.9727\n",
            "val Loss: 0.2332 Acc: 0.9293\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 11/30\n",
            "----------\n",
            "train Loss: 0.0416 Acc: 0.9869\n",
            "val Loss: 0.1498 Acc: 0.9647\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch 12/30\n",
            "----------\n",
            "train Loss: 0.0368 Acc: 0.9893\n",
            "val Loss: 0.1256 Acc: 0.9700\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = test_model(device, resnext50d_tf, test_loader)"
      ],
      "metadata": {
        "id": "Q-d4P0RIa9AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss.loc[:, ['blast', 'brown', 'healthy']] = test_pred\n",
        "ss.to_csv(\"result_resnext50d_aug.csv\", index=False)\n",
        "ss.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jbiXk7AQbEWj",
        "outputId": "5a396a85-835e-475a-9378-38cf48c3e33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Image_id     blast         brown       healthy\n",
              "0  id_00vl5wvxq3.jpg  0.999962  1.767281e-07  3.809216e-05\n",
              "1  id_01hu05mtch.jpg  0.000003  9.999720e-01  2.532256e-05\n",
              "2  id_030ln10ewn.jpg  0.175352  8.230674e-01  1.580324e-03\n",
              "3  id_03z57m8xht.jpg  1.000000  7.598925e-09  1.117548e-08\n",
              "4  id_04ngep1w4b.jpg  0.997751  2.247754e-03  1.492717e-06"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae4b9c76-5dcf-4007-92fd-f404cdc67d53\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_id</th>\n",
              "      <th>blast</th>\n",
              "      <th>brown</th>\n",
              "      <th>healthy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_00vl5wvxq3.jpg</td>\n",
              "      <td>0.999962</td>\n",
              "      <td>1.767281e-07</td>\n",
              "      <td>3.809216e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_01hu05mtch.jpg</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>9.999720e-01</td>\n",
              "      <td>2.532256e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_030ln10ewn.jpg</td>\n",
              "      <td>0.175352</td>\n",
              "      <td>8.230674e-01</td>\n",
              "      <td>1.580324e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_03z57m8xht.jpg</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.598925e-09</td>\n",
              "      <td>1.117548e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_04ngep1w4b.jpg</td>\n",
              "      <td>0.997751</td>\n",
              "      <td>2.247754e-03</td>\n",
              "      <td>1.492717e-06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae4b9c76-5dcf-4007-92fd-f404cdc67d53')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae4b9c76-5dcf-4007-92fd-f404cdc67d53 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae4b9c76-5dcf-4007-92fd-f404cdc67d53');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "score : 0.15 (SOTA !!)"
      ],
      "metadata": {
        "id": "z6AV1jq-bUx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# resnext101_32x8d"
      ],
      "metadata": {
        "id": "ztIvwCY-bYQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = create_model('resnext101_32x8d', pretrained=True)\n",
        "print(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMtjfnQ5b-1_",
        "outputId": "f6eeecfc-ff0b-419e-c133-2ce3a01e5c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\" to /root/.cache/torch/hub/checkpoints/resnext101_32x8d-8ba56ff5.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (act1): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (12): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (13): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (14): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (15): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (16): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (17): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (18): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (19): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (20): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (21): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (22): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myResNext101(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(myResNext101, self).__init__()\n",
        "    self.model_ft = create_model('resnext101_32x8d', pretrained=True)\n",
        "    \n",
        "    num_ftrs = self.model_ft.fc.in_features\n",
        "    self.model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.model_ft(x)\n",
        "      return out"
      ],
      "metadata": {
        "id": "fPQ6S7lpcFqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnext101_tf = myResNext101(3)"
      ],
      "metadata": {
        "id": "hKI85C3EcS6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "# train_loader, val_loader, test_loader = make_data_loader_with_aug(batch_size=32, size=224)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = optim.Adam(resnext101_tf.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "MODEL_DIR = '/content/gdrive/MyDrive/ML/MRDC-competition/models'\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.00001, path=os.path.join(MODEL_DIR, \"resnext101_aug_checkpoint.pt\"))"
      ],
      "metadata": {
        "id": "EnOAGjaWcXHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnext101_tf, val_label, val_pred = train_model(device, resnext101_tf, train_loader, val_loader, criterion, optimizer, 30, early_stopping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj269Dyrcdns",
        "outputId": "f005ba51-afc3-410e-bb08-5e9ba0249e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "train Loss: 0.5143 Acc: 0.7972\n",
            "val Loss: 2.4462 Acc: 0.7901\n",
            "Validation loss decreased (inf --> 2.446233).  Saving model ...\n",
            "Epoch 2/30\n",
            "----------\n",
            "train Loss: 0.2295 Acc: 0.9207\n",
            "val Loss: 0.4034 Acc: 0.8640\n",
            "Validation loss decreased (2.446233 --> 0.403411).  Saving model ...\n",
            "Epoch 3/30\n",
            "----------\n",
            "train Loss: 0.1376 Acc: 0.9555\n",
            "val Loss: 0.2570 Acc: 0.9058\n",
            "Validation loss decreased (0.403411 --> 0.257003).  Saving model ...\n",
            "Epoch 4/30\n",
            "----------\n",
            "train Loss: 0.1193 Acc: 0.9571\n",
            "val Loss: 0.1838 Acc: 0.9486\n",
            "Validation loss decreased (0.257003 --> 0.183843).  Saving model ...\n",
            "Epoch 5/30\n",
            "----------\n",
            "train Loss: 0.0714 Acc: 0.9788\n",
            "val Loss: 0.2047 Acc: 0.9283\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 6/30\n",
            "----------\n",
            "train Loss: 0.1040 Acc: 0.9649\n",
            "val Loss: 0.2416 Acc: 0.9283\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 7/30\n",
            "----------\n",
            "train Loss: 0.0555 Acc: 0.9802\n",
            "val Loss: 0.2501 Acc: 0.9347\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 8/30\n",
            "----------\n",
            "train Loss: 0.0701 Acc: 0.9743\n",
            "val Loss: 0.2554 Acc: 0.9293\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch 9/30\n",
            "----------\n",
            "train Loss: 0.0562 Acc: 0.9788\n",
            "val Loss: 0.2101 Acc: 0.9433\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = test_model(device, resnext101_tf, test_loader)"
      ],
      "metadata": {
        "id": "nL5GeTKggfts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss.loc[:, ['blast', 'brown', 'healthy']] = test_pred\n",
        "ss.to_csv(\"result_resnext101_aug.csv\", index=False)\n",
        "ss.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IB4B7k4HgjLl",
        "outputId": "75fbe601-65b1-49bd-9b66-84b7299ebdec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Image_id     blast         brown       healthy\n",
              "0  id_00vl5wvxq3.jpg  0.999959  2.420183e-05  1.632305e-05\n",
              "1  id_01hu05mtch.jpg  0.000431  9.966742e-01  2.895207e-03\n",
              "2  id_030ln10ewn.jpg  0.825550  1.377005e-01  3.674957e-02\n",
              "3  id_03z57m8xht.jpg  1.000000  1.441248e-07  1.858684e-08\n",
              "4  id_04ngep1w4b.jpg  0.994193  5.440183e-03  3.669000e-04"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-174bd676-165b-489a-b62f-3e8de0afe620\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_id</th>\n",
              "      <th>blast</th>\n",
              "      <th>brown</th>\n",
              "      <th>healthy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_00vl5wvxq3.jpg</td>\n",
              "      <td>0.999959</td>\n",
              "      <td>2.420183e-05</td>\n",
              "      <td>1.632305e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_01hu05mtch.jpg</td>\n",
              "      <td>0.000431</td>\n",
              "      <td>9.966742e-01</td>\n",
              "      <td>2.895207e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_030ln10ewn.jpg</td>\n",
              "      <td>0.825550</td>\n",
              "      <td>1.377005e-01</td>\n",
              "      <td>3.674957e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_03z57m8xht.jpg</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.441248e-07</td>\n",
              "      <td>1.858684e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_04ngep1w4b.jpg</td>\n",
              "      <td>0.994193</td>\n",
              "      <td>5.440183e-03</td>\n",
              "      <td>3.669000e-04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-174bd676-165b-489a-b62f-3e8de0afe620')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-174bd676-165b-489a-b62f-3e8de0afe620 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-174bd676-165b-489a-b62f-3e8de0afe620');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "score : 0.29"
      ],
      "metadata": {
        "id": "eA2KXbStg2um"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConvNext Tiny (timm ver.)"
      ],
      "metadata": {
        "id": "qnsb08YAVtxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myConvNextTiny(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super( myConvNextTiny, self).__init__()\n",
        "    self.model_ft = create_model('convnext_tiny', pretrained=True)\n",
        "    \n",
        "    num_ftrs = self.model_ft.head.fc.in_features\n",
        "    self.model_ft.head.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.model_ft(x)\n",
        "      return out"
      ],
      "metadata": {
        "id": "9ON8lEw6VvsK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convnext_tiny_tf = myConvNextTiny(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_zDlBhpV1mr",
        "outputId": "837eb963-e322-4493-dfd5-463a48b7601c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny_1k_224_ema.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "train_loader, val_loader, test_loader = make_data_loader_with_aug(batch_size=128, size=224)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = optim.Adam(convnext_tiny_tf.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "MODEL_DIR = '/content/gdrive/MyDrive/ML/MRDC-competition/models'\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.00001, path=os.path.join(MODEL_DIR, \"convnext_tiny_aug_checkpoint.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ5gcpGvV-mJ",
        "outputId": "4d4e38de-103a-4ead-e536-916273c20efa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "1232\n",
            "1233\n",
            "1234\n",
            "1235\n",
            "1236\n",
            "1237\n",
            "1238\n",
            "1239\n",
            "1240\n",
            "1241\n",
            "1242\n",
            "1243\n",
            "1244\n",
            "1245\n",
            "1246\n",
            "1247\n",
            "1248\n",
            "1249\n",
            "1250\n",
            "1251\n",
            "1252\n",
            "1253\n",
            "1254\n",
            "1255\n",
            "1256\n",
            "1257\n",
            "1258\n",
            "1259\n",
            "1260\n",
            "1261\n",
            "1262\n",
            "1263\n",
            "1264\n",
            "1265\n",
            "1266\n",
            "1267\n",
            "1268\n",
            "1269\n",
            "1270\n",
            "1271\n",
            "1272\n",
            "1273\n",
            "1274\n",
            "1275\n",
            "1276\n",
            "1277\n",
            "1278\n",
            "1279\n",
            "1280\n",
            "1281\n",
            "1282\n",
            "1283\n",
            "1284\n",
            "1285\n",
            "1286\n",
            "1287\n",
            "1288\n",
            "1289\n",
            "1290\n",
            "1291\n",
            "1292\n",
            "1293\n",
            "1294\n",
            "1295\n",
            "1296\n",
            "1297\n",
            "1298\n",
            "1299\n",
            "1300\n",
            "1301\n",
            "1302\n",
            "1303\n",
            "1304\n",
            "1305\n",
            "1306\n",
            "1307\n",
            "1308\n",
            "1309\n",
            "1310\n",
            "1311\n",
            "1312\n",
            "1313\n",
            "1314\n",
            "1315\n",
            "1316\n",
            "1317\n",
            "1318\n",
            "1319\n",
            "1320\n",
            "1321\n",
            "1322\n",
            "1323\n",
            "1324\n",
            "1325\n",
            "1326\n",
            "1327\n",
            "1328\n",
            "1329\n",
            "1330\n",
            "1331\n",
            "1332\n",
            "1333\n",
            "1334\n",
            "1335\n",
            "1336\n",
            "1337\n",
            "1338\n",
            "1339\n",
            "1340\n",
            "1341\n",
            "1342\n",
            "1343\n",
            "1344\n",
            "1345\n",
            "1346\n",
            "1347\n",
            "1348\n",
            "1349\n",
            "1350\n",
            "1351\n",
            "1352\n",
            "1353\n",
            "1354\n",
            "1355\n",
            "1356\n",
            "1357\n",
            "1358\n",
            "1359\n",
            "1360\n",
            "1361\n",
            "1362\n",
            "1363\n",
            "1364\n",
            "1365\n",
            "1366\n",
            "1367\n",
            "1368\n",
            "1369\n",
            "1370\n",
            "1371\n",
            "1372\n",
            "1373\n",
            "1374\n",
            "1375\n",
            "1376\n",
            "1377\n",
            "1378\n",
            "1379\n",
            "1380\n",
            "1381\n",
            "1382\n",
            "1383\n",
            "1384\n",
            "1385\n",
            "1386\n",
            "1387\n",
            "1388\n",
            "1389\n",
            "1390\n",
            "1391\n",
            "1392\n",
            "1393\n",
            "1394\n",
            "1395\n",
            "1396\n",
            "1397\n",
            "1398\n",
            "1399\n",
            "1400\n",
            "1401\n",
            "1402\n",
            "1403\n",
            "1404\n",
            "1405\n",
            "1406\n",
            "1407\n",
            "1408\n",
            "1409\n",
            "1410\n",
            "1411\n",
            "1412\n",
            "1413\n",
            "1414\n",
            "1415\n",
            "1416\n",
            "1417\n",
            "1418\n",
            "1419\n",
            "1420\n",
            "1421\n",
            "1422\n",
            "1423\n",
            "1424\n",
            "1425\n",
            "1426\n",
            "1427\n",
            "1428\n",
            "1429\n",
            "1430\n",
            "1431\n",
            "1432\n",
            "1433\n",
            "1434\n",
            "1435\n",
            "1436\n",
            "1437\n",
            "1438\n",
            "1439\n",
            "1440\n",
            "1441\n",
            "1442\n",
            "1443\n",
            "1444\n",
            "1445\n",
            "1446\n",
            "1447\n",
            "1448\n",
            "1449\n",
            "1450\n",
            "1451\n",
            "1452\n",
            "1453\n",
            "1454\n",
            "1455\n",
            "1456\n",
            "1457\n",
            "1458\n",
            "1459\n",
            "1460\n",
            "1461\n",
            "1462\n",
            "1463\n",
            "1464\n",
            "1465\n",
            "1466\n",
            "1467\n",
            "1468\n",
            "1469\n",
            "1470\n",
            "1471\n",
            "1472\n",
            "1473\n",
            "1474\n",
            "1475\n",
            "1476\n",
            "1477\n",
            "1478\n",
            "1479\n",
            "1480\n",
            "1481\n",
            "1482\n",
            "1483\n",
            "1484\n",
            "1485\n",
            "1486\n",
            "1487\n",
            "1488\n",
            "1489\n",
            "1490\n",
            "1491\n",
            "1492\n",
            "1493\n",
            "1494\n",
            "1495\n",
            "1496\n",
            "1497\n",
            "1498\n",
            "1499\n",
            "1500\n",
            "1501\n",
            "1502\n",
            "1503\n",
            "1504\n",
            "1505\n",
            "1506\n",
            "1507\n",
            "1508\n",
            "1509\n",
            "1510\n",
            "1511\n",
            "1512\n",
            "1513\n",
            "1514\n",
            "1515\n",
            "1516\n",
            "1517\n",
            "1518\n",
            "1519\n",
            "1520\n",
            "1521\n",
            "1522\n",
            "1523\n",
            "1524\n",
            "1525\n",
            "1526\n",
            "1527\n",
            "1528\n",
            "1529\n",
            "1530\n",
            "1531\n",
            "1532\n",
            "1533\n",
            "1534\n",
            "1535\n",
            "1536\n",
            "1537\n",
            "1538\n",
            "1539\n",
            "1540\n",
            "1541\n",
            "1542\n",
            "1543\n",
            "1544\n",
            "1545\n",
            "1546\n",
            "1547\n",
            "1548\n",
            "1549\n",
            "1550\n",
            "1551\n",
            "1552\n",
            "1553\n",
            "1554\n",
            "1555\n",
            "1556\n",
            "1557\n",
            "1558\n",
            "1559\n",
            "1560\n",
            "1561\n",
            "1562\n",
            "1563\n",
            "1564\n",
            "1565\n",
            "1566\n",
            "1567\n",
            "1568\n",
            "1569\n",
            "1570\n",
            "1571\n",
            "1572\n",
            "1573\n",
            "1574\n",
            "1575\n",
            "1576\n",
            "1577\n",
            "1578\n",
            "1579\n",
            "1580\n",
            "1581\n",
            "1582\n",
            "1583\n",
            "1584\n",
            "1585\n",
            "1586\n",
            "1587\n",
            "1588\n",
            "1589\n",
            "1590\n",
            "1591\n",
            "1592\n",
            "1593\n",
            "1594\n",
            "1595\n",
            "1596\n",
            "1597\n",
            "1598\n",
            "1599\n",
            "1600\n",
            "1601\n",
            "1602\n",
            "1603\n",
            "1604\n",
            "1605\n",
            "1606\n",
            "1607\n",
            "1608\n",
            "1609\n",
            "1610\n",
            "1611\n",
            "1612\n",
            "1613\n",
            "1614\n",
            "1615\n",
            "1616\n",
            "1617\n",
            "1618\n",
            "1619\n",
            "1620\n",
            "1621\n",
            "1622\n",
            "1623\n",
            "1624\n",
            "1625\n",
            "1626\n",
            "1627\n",
            "1628\n",
            "1629\n",
            "1630\n",
            "1631\n",
            "1632\n",
            "1633\n",
            "1634\n",
            "1635\n",
            "1636\n",
            "1637\n",
            "1638\n",
            "1639\n",
            "1640\n",
            "1641\n",
            "1642\n",
            "1643\n",
            "1644\n",
            "1645\n",
            "1646\n",
            "1647\n",
            "1648\n",
            "1649\n",
            "1650\n",
            "1651\n",
            "1652\n",
            "1653\n",
            "1654\n",
            "1655\n",
            "1656\n",
            "1657\n",
            "1658\n",
            "1659\n",
            "1660\n",
            "1661\n",
            "1662\n",
            "1663\n",
            "1664\n",
            "1665\n",
            "1666\n",
            "1667\n",
            "1668\n",
            "1669\n",
            "1670\n",
            "1671\n",
            "1672\n",
            "1673\n",
            "1674\n",
            "1675\n",
            "1676\n",
            "1677\n",
            "1678\n",
            "1679\n",
            "1680\n",
            "1681\n",
            "1682\n",
            "1683\n",
            "1684\n",
            "1685\n",
            "1686\n",
            "1687\n",
            "1688\n",
            "1689\n",
            "1690\n",
            "1691\n",
            "1692\n",
            "1693\n",
            "1694\n",
            "1695\n",
            "1696\n",
            "1697\n",
            "1698\n",
            "1699\n",
            "1700\n",
            "1701\n",
            "1702\n",
            "1703\n",
            "1704\n",
            "1705\n",
            "1706\n",
            "1707\n",
            "1708\n",
            "1709\n",
            "1710\n",
            "1711\n",
            "1712\n",
            "1713\n",
            "1714\n",
            "1715\n",
            "1716\n",
            "1717\n",
            "1718\n",
            "1719\n",
            "1720\n",
            "1721\n",
            "1722\n",
            "1723\n",
            "1724\n",
            "1725\n",
            "1726\n",
            "1727\n",
            "1728\n",
            "1729\n",
            "1730\n",
            "1731\n",
            "1732\n",
            "1733\n",
            "1734\n",
            "1735\n",
            "1736\n",
            "1737\n",
            "1738\n",
            "1739\n",
            "1740\n",
            "1741\n",
            "1742\n",
            "1743\n",
            "1744\n",
            "1745\n",
            "1746\n",
            "1747\n",
            "1748\n",
            "1749\n",
            "1750\n",
            "1751\n",
            "1752\n",
            "1753\n",
            "1754\n",
            "1755\n",
            "1756\n",
            "1757\n",
            "1758\n",
            "1759\n",
            "1760\n",
            "1761\n",
            "1762\n",
            "1763\n",
            "1764\n",
            "1765\n",
            "1766\n",
            "1767\n",
            "1768\n",
            "1769\n",
            "1770\n",
            "1771\n",
            "1772\n",
            "1773\n",
            "1774\n",
            "1775\n",
            "1776\n",
            "1777\n",
            "1778\n",
            "1779\n",
            "1780\n",
            "1781\n",
            "1782\n",
            "1783\n",
            "1784\n",
            "1785\n",
            "1786\n",
            "1787\n",
            "1788\n",
            "1789\n",
            "1790\n",
            "1791\n",
            "1792\n",
            "1793\n",
            "1794\n",
            "1795\n",
            "1796\n",
            "1797\n",
            "1798\n",
            "1799\n",
            "1800\n",
            "1801\n",
            "1802\n",
            "1803\n",
            "1804\n",
            "1805\n",
            "1806\n",
            "1807\n",
            "1808\n",
            "1809\n",
            "1810\n",
            "1811\n",
            "1812\n",
            "1813\n",
            "1814\n",
            "1815\n",
            "1816\n",
            "1817\n",
            "1818\n",
            "1819\n",
            "1820\n",
            "1821\n",
            "1822\n",
            "1823\n",
            "1824\n",
            "1825\n",
            "1826\n",
            "1827\n",
            "1828\n",
            "1829\n",
            "1830\n",
            "1831\n",
            "1832\n",
            "1833\n",
            "1834\n",
            "1835\n",
            "1836\n",
            "1837\n",
            "1838\n",
            "1839\n",
            "1840\n",
            "1841\n",
            "1842\n",
            "1843\n",
            "1844\n",
            "1845\n",
            "1846\n",
            "1847\n",
            "1848\n",
            "1849\n",
            "1850\n",
            "1851\n",
            "1852\n",
            "1853\n",
            "1854\n",
            "1855\n",
            "1856\n",
            "1857\n",
            "1858\n",
            "1859\n",
            "1860\n",
            "1861\n",
            "1862\n",
            "1863\n",
            "1864\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1868\n",
            "1869\n",
            "1870\n",
            "1871\n",
            "1872\n",
            "1873\n",
            "1874\n",
            "1875\n",
            "1876\n",
            "1877\n",
            "1878\n",
            "1879\n",
            "1880\n",
            "1881\n",
            "1882\n",
            "1883\n",
            "1884\n",
            "1885\n",
            "1886\n",
            "1887\n",
            "1888\n",
            "1889\n",
            "1890\n",
            "1891\n",
            "1892\n",
            "1893\n",
            "1894\n",
            "1895\n",
            "1896\n",
            "1897\n",
            "1898\n",
            "1899\n",
            "1900\n",
            "1901\n",
            "1902\n",
            "1903\n",
            "1904\n",
            "1905\n",
            "1906\n",
            "1907\n",
            "1908\n",
            "1909\n",
            "1910\n",
            "1911\n",
            "1912\n",
            "1913\n",
            "1914\n",
            "1915\n",
            "1916\n",
            "1917\n",
            "1918\n",
            "1919\n",
            "1920\n",
            "1921\n",
            "1922\n",
            "1923\n",
            "1924\n",
            "1925\n",
            "1926\n",
            "1927\n",
            "1928\n",
            "1929\n",
            "1930\n",
            "1931\n",
            "1932\n",
            "1933\n",
            "1934\n",
            "1935\n",
            "1936\n",
            "1937\n",
            "1938\n",
            "1939\n",
            "1940\n",
            "1941\n",
            "1942\n",
            "1943\n",
            "1944\n",
            "1945\n",
            "1946\n",
            "1947\n",
            "1948\n",
            "1949\n",
            "1950\n",
            "1951\n",
            "1952\n",
            "1953\n",
            "1954\n",
            "1955\n",
            "1956\n",
            "1957\n",
            "1958\n",
            "1959\n",
            "1960\n",
            "1961\n",
            "1962\n",
            "1963\n",
            "1964\n",
            "1965\n",
            "1966\n",
            "1967\n",
            "1968\n",
            "1969\n",
            "1970\n",
            "1971\n",
            "1972\n",
            "1973\n",
            "1974\n",
            "1975\n",
            "1976\n",
            "1977\n",
            "1978\n",
            "1979\n",
            "1980\n",
            "1981\n",
            "1982\n",
            "1983\n",
            "1984\n",
            "1985\n",
            "1986\n",
            "1987\n",
            "1988\n",
            "1989\n",
            "1990\n",
            "1991\n",
            "1992\n",
            "1993\n",
            "1994\n",
            "1995\n",
            "1996\n",
            "1997\n",
            "1998\n",
            "1999\n",
            "2000\n",
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "2025\n",
            "2026\n",
            "2027\n",
            "2028\n",
            "2029\n",
            "2030\n",
            "2031\n",
            "2032\n",
            "2033\n",
            "2034\n",
            "2035\n",
            "2036\n",
            "2037\n",
            "2038\n",
            "2039\n",
            "2040\n",
            "2041\n",
            "2042\n",
            "2043\n",
            "2044\n",
            "2045\n",
            "2046\n",
            "2047\n",
            "2048\n",
            "2049\n",
            "2050\n",
            "2051\n",
            "2052\n",
            "2053\n",
            "2054\n",
            "2055\n",
            "2056\n",
            "2057\n",
            "2058\n",
            "2059\n",
            "2060\n",
            "2061\n",
            "2062\n",
            "2063\n",
            "2064\n",
            "2065\n",
            "2066\n",
            "2067\n",
            "2068\n",
            "2069\n",
            "2070\n",
            "2071\n",
            "2072\n",
            "2073\n",
            "2074\n",
            "2075\n",
            "2076\n",
            "2077\n",
            "2078\n",
            "2079\n",
            "2080\n",
            "2081\n",
            "2082\n",
            "2083\n",
            "2084\n",
            "2085\n",
            "2086\n",
            "2087\n",
            "2088\n",
            "2089\n",
            "2090\n",
            "2091\n",
            "2092\n",
            "2093\n",
            "2094\n",
            "2095\n",
            "2096\n",
            "2097\n",
            "2098\n",
            "2099\n",
            "2100\n",
            "2101\n",
            "2102\n",
            "2103\n",
            "2104\n",
            "2105\n",
            "2106\n",
            "2107\n",
            "2108\n",
            "2109\n",
            "2110\n",
            "2111\n",
            "2112\n",
            "2113\n",
            "2114\n",
            "2115\n",
            "2116\n",
            "2117\n",
            "2118\n",
            "2119\n",
            "2120\n",
            "2121\n",
            "2122\n",
            "2123\n",
            "2124\n",
            "2125\n",
            "2126\n",
            "2127\n",
            "2128\n",
            "2129\n",
            "2130\n",
            "2131\n",
            "2132\n",
            "2133\n",
            "2134\n",
            "2135\n",
            "2136\n",
            "2137\n",
            "2138\n",
            "2139\n",
            "2140\n",
            "2141\n",
            "2142\n",
            "2143\n",
            "2144\n",
            "2145\n",
            "2146\n",
            "2147\n",
            "2148\n",
            "2149\n",
            "2150\n",
            "2151\n",
            "2152\n",
            "2153\n",
            "2154\n",
            "2155\n",
            "2156\n",
            "2157\n",
            "2158\n",
            "2159\n",
            "2160\n",
            "2161\n",
            "2162\n",
            "2163\n",
            "2164\n",
            "2165\n",
            "2166\n",
            "2167\n",
            "2168\n",
            "2169\n",
            "2170\n",
            "2171\n",
            "2172\n",
            "2173\n",
            "2174\n",
            "2175\n",
            "2176\n",
            "2177\n",
            "2178\n",
            "2179\n",
            "2180\n",
            "2181\n",
            "2182\n",
            "2183\n",
            "2184\n",
            "2185\n",
            "2186\n",
            "2187\n",
            "2188\n",
            "2189\n",
            "2190\n",
            "2191\n",
            "2192\n",
            "2193\n",
            "2194\n",
            "2195\n",
            "2196\n",
            "2197\n",
            "2198\n",
            "2199\n",
            "2200\n",
            "2201\n",
            "2202\n",
            "2203\n",
            "2204\n",
            "2205\n",
            "2206\n",
            "2207\n",
            "2208\n",
            "2209\n",
            "2210\n",
            "2211\n",
            "2212\n",
            "2213\n",
            "2214\n",
            "2215\n",
            "2216\n",
            "2217\n",
            "2218\n",
            "2219\n",
            "2220\n",
            "2221\n",
            "2222\n",
            "2223\n",
            "2224\n",
            "2225\n",
            "2226\n",
            "2227\n",
            "2228\n",
            "2229\n",
            "2230\n",
            "2231\n",
            "2232\n",
            "2233\n",
            "2234\n",
            "2235\n",
            "2236\n",
            "2237\n",
            "2238\n",
            "2239\n",
            "2240\n",
            "2241\n",
            "2242\n",
            "2243\n",
            "2244\n",
            "2245\n",
            "2246\n",
            "2247\n",
            "2248\n",
            "2249\n",
            "2250\n",
            "2251\n",
            "2252\n",
            "2253\n",
            "2254\n",
            "2255\n",
            "2256\n",
            "2257\n",
            "2258\n",
            "2259\n",
            "2260\n",
            "2261\n",
            "2262\n",
            "2263\n",
            "2264\n",
            "2265\n",
            "2266\n",
            "2267\n",
            "2268\n",
            "2269\n",
            "2270\n",
            "2271\n",
            "2272\n",
            "2273\n",
            "2274\n",
            "2275\n",
            "2276\n",
            "2277\n",
            "2278\n",
            "2279\n",
            "2280\n",
            "2281\n",
            "2282\n",
            "2283\n",
            "2284\n",
            "2285\n",
            "2286\n",
            "2287\n",
            "2288\n",
            "2289\n",
            "2290\n",
            "2291\n",
            "2292\n",
            "2293\n",
            "2294\n",
            "2295\n",
            "2296\n",
            "2297\n",
            "2298\n",
            "2299\n",
            "2300\n",
            "2301\n",
            "2302\n",
            "2303\n",
            "2304\n",
            "2305\n",
            "2306\n",
            "2307\n",
            "2308\n",
            "2309\n",
            "2310\n",
            "2311\n",
            "2312\n",
            "2313\n",
            "2314\n",
            "2315\n",
            "2316\n",
            "2317\n",
            "2318\n",
            "2319\n",
            "2320\n",
            "2321\n",
            "2322\n",
            "2323\n",
            "2324\n",
            "2325\n",
            "2326\n",
            "2327\n",
            "2328\n",
            "2329\n",
            "2330\n",
            "2331\n",
            "2332\n",
            "2333\n",
            "2334\n",
            "2335\n",
            "2336\n",
            "2337\n",
            "2338\n",
            "2339\n",
            "2340\n",
            "2341\n",
            "2342\n",
            "2343\n",
            "2344\n",
            "2345\n",
            "2346\n",
            "2347\n",
            "2348\n",
            "2349\n",
            "2350\n",
            "2351\n",
            "2352\n",
            "2353\n",
            "2354\n",
            "2355\n",
            "2356\n",
            "2357\n",
            "2358\n",
            "2359\n",
            "2360\n",
            "2361\n",
            "2362\n",
            "2363\n",
            "2364\n",
            "2365\n",
            "2366\n",
            "2367\n",
            "2368\n",
            "2369\n",
            "2370\n",
            "2371\n",
            "2372\n",
            "2373\n",
            "2374\n",
            "2375\n",
            "2376\n",
            "2377\n",
            "2378\n",
            "2379\n",
            "2380\n",
            "2381\n",
            "2382\n",
            "2383\n",
            "2384\n",
            "2385\n",
            "2386\n",
            "2387\n",
            "2388\n",
            "2389\n",
            "2390\n",
            "2391\n",
            "2392\n",
            "2393\n",
            "2394\n",
            "2395\n",
            "2396\n",
            "2397\n",
            "2398\n",
            "2399\n",
            "2400\n",
            "2401\n",
            "2402\n",
            "2403\n",
            "2404\n",
            "2405\n",
            "2406\n",
            "2407\n",
            "2408\n",
            "2409\n",
            "2410\n",
            "2411\n",
            "2412\n",
            "2413\n",
            "2414\n",
            "2415\n",
            "2416\n",
            "2417\n",
            "2418\n",
            "2419\n",
            "2420\n",
            "2421\n",
            "2422\n",
            "2423\n",
            "2424\n",
            "2425\n",
            "2426\n",
            "2427\n",
            "2428\n",
            "2429\n",
            "2430\n",
            "2431\n",
            "2432\n",
            "2433\n",
            "2434\n",
            "2435\n",
            "2436\n",
            "2437\n",
            "2438\n",
            "2439\n",
            "2440\n",
            "2441\n",
            "2442\n",
            "2443\n",
            "2444\n",
            "2445\n",
            "2446\n",
            "2447\n",
            "2448\n",
            "2449\n",
            "2450\n",
            "2451\n",
            "2452\n",
            "2453\n",
            "2454\n",
            "2455\n",
            "2456\n",
            "2457\n",
            "2458\n",
            "2459\n",
            "2460\n",
            "2461\n",
            "2462\n",
            "2463\n",
            "2464\n",
            "2465\n",
            "2466\n",
            "2467\n",
            "2468\n",
            "2469\n",
            "2470\n",
            "2471\n",
            "2472\n",
            "2473\n",
            "2474\n",
            "2475\n",
            "2476\n",
            "2477\n",
            "2478\n",
            "2479\n",
            "2480\n",
            "2481\n",
            "2482\n",
            "2483\n",
            "2484\n",
            "2485\n",
            "2486\n",
            "2487\n",
            "2488\n",
            "2489\n",
            "2490\n",
            "2491\n",
            "2492\n",
            "2493\n",
            "2494\n",
            "2495\n",
            "2496\n",
            "2497\n",
            "2498\n",
            "2499\n",
            "2500\n",
            "2501\n",
            "2502\n",
            "2503\n",
            "2504\n",
            "2505\n",
            "2506\n",
            "2507\n",
            "2508\n",
            "2509\n",
            "2510\n",
            "2511\n",
            "2512\n",
            "2513\n",
            "2514\n",
            "2515\n",
            "2516\n",
            "2517\n",
            "2518\n",
            "2519\n",
            "2520\n",
            "2521\n",
            "2522\n",
            "2523\n",
            "2524\n",
            "2525\n",
            "2526\n",
            "2527\n",
            "2528\n",
            "2529\n",
            "2530\n",
            "2531\n",
            "2532\n",
            "2533\n",
            "2534\n",
            "2535\n",
            "2536\n",
            "2537\n",
            "2538\n",
            "2539\n",
            "2540\n",
            "2541\n",
            "2542\n",
            "2543\n",
            "2544\n",
            "2545\n",
            "2546\n",
            "2547\n",
            "2548\n",
            "2549\n",
            "2550\n",
            "2551\n",
            "2552\n",
            "2553\n",
            "2554\n",
            "2555\n",
            "2556\n",
            "2557\n",
            "2558\n",
            "2559\n",
            "2560\n",
            "2561\n",
            "2562\n",
            "2563\n",
            "2564\n",
            "2565\n",
            "2566\n",
            "2567\n",
            "2568\n",
            "2569\n",
            "2570\n",
            "2571\n",
            "2572\n",
            "2573\n",
            "2574\n",
            "2575\n",
            "2576\n",
            "2577\n",
            "2578\n",
            "2579\n",
            "2580\n",
            "2581\n",
            "2582\n",
            "2583\n",
            "2584\n",
            "2585\n",
            "2586\n",
            "2587\n",
            "2588\n",
            "2589\n",
            "2590\n",
            "2591\n",
            "2592\n",
            "2593\n",
            "2594\n",
            "2595\n",
            "2596\n",
            "2597\n",
            "2598\n",
            "2599\n",
            "2600\n",
            "2601\n",
            "2602\n",
            "2603\n",
            "2604\n",
            "2605\n",
            "2606\n",
            "2607\n",
            "2608\n",
            "2609\n",
            "2610\n",
            "2611\n",
            "2612\n",
            "2613\n",
            "2614\n",
            "2615\n",
            "2616\n",
            "2617\n",
            "2618\n",
            "2619\n",
            "2620\n",
            "2621\n",
            "2622\n",
            "2623\n",
            "2624\n",
            "2625\n",
            "2626\n",
            "2627\n",
            "2628\n",
            "2629\n",
            "2630\n",
            "2631\n",
            "2632\n",
            "2633\n",
            "2634\n",
            "2635\n",
            "2636\n",
            "2637\n",
            "2638\n",
            "2639\n",
            "2640\n",
            "2641\n",
            "2642\n",
            "2643\n",
            "2644\n",
            "2645\n",
            "2646\n",
            "2647\n",
            "2648\n",
            "2649\n",
            "2650\n",
            "2651\n",
            "2652\n",
            "2653\n",
            "2654\n",
            "2655\n",
            "2656\n",
            "2657\n",
            "2658\n",
            "2659\n",
            "2660\n",
            "2661\n",
            "2662\n",
            "2663\n",
            "2664\n",
            "2665\n",
            "2666\n",
            "2667\n",
            "2668\n",
            "2669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convnext_tiny_tf, val_label, val_pred = train_model(device, convnext_tiny_tf, train_loader, val_loader, criterion, optimizer, 30, early_stopping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VQ6wsxgixO0",
        "outputId": "43886029-d224-47a7-9881-9dfc8ff607a4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "train Loss: 0.5846 Acc: 0.7720\n",
            "val Loss: 0.3081 Acc: 0.8983\n",
            "Validation loss decreased (inf --> 0.308103).  Saving model ...\n",
            "Epoch 2/30\n",
            "----------\n",
            "train Loss: 0.1232 Acc: 0.9636\n",
            "val Loss: 0.1533 Acc: 0.9518\n",
            "Validation loss decreased (0.308103 --> 0.153276).  Saving model ...\n",
            "Epoch 3/30\n",
            "----------\n",
            "train Loss: 0.0200 Acc: 0.9954\n",
            "val Loss: 0.1046 Acc: 0.9604\n",
            "Validation loss decreased (0.153276 --> 0.104606).  Saving model ...\n",
            "Epoch 4/30\n",
            "----------\n",
            "train Loss: 0.0045 Acc: 0.9997\n",
            "val Loss: 0.1673 Acc: 0.9529\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 5/30\n",
            "----------\n",
            "train Loss: 0.0014 Acc: 1.0000\n",
            "val Loss: 0.1142 Acc: 0.9657\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 6/30\n",
            "----------\n",
            "train Loss: 0.0004 Acc: 1.0000\n",
            "val Loss: 0.1281 Acc: 0.9690\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 7/30\n",
            "----------\n",
            "train Loss: 0.0003 Acc: 1.0000\n",
            "val Loss: 0.1298 Acc: 0.9657\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch 8/30\n",
            "----------\n",
            "train Loss: 0.0002 Acc: 1.0000\n",
            "val Loss: 0.1357 Acc: 0.9657\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = test_model(device, convnext_tiny_tf, test_loader)"
      ],
      "metadata": {
        "id": "2E1VLtX4pZ_P"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss.loc[:, ['blast', 'brown', 'healthy']] = test_pred\n",
        "ss.to_csv(\"result_convnext_tiny_timm_ver_aug.csv\", index=False)\n",
        "ss.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "OqeJTee4pg_-",
        "outputId": "a1ea566b-b2ff-4cf7-c193-5ff000b3fcf9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Image_id     blast     brown   healthy\n",
              "0  id_00vl5wvxq3.jpg  0.999307  0.000347  0.000346\n",
              "1  id_01hu05mtch.jpg  0.045972  0.953730  0.000298\n",
              "2  id_030ln10ewn.jpg  0.941781  0.043251  0.014968\n",
              "3  id_03z57m8xht.jpg  0.999697  0.000248  0.000055\n",
              "4  id_04ngep1w4b.jpg  0.998063  0.001426  0.000511"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c5c8b01e-bc6e-429e-84cb-0b5e632e4184\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_id</th>\n",
              "      <th>blast</th>\n",
              "      <th>brown</th>\n",
              "      <th>healthy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_00vl5wvxq3.jpg</td>\n",
              "      <td>0.999307</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>0.000346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_01hu05mtch.jpg</td>\n",
              "      <td>0.045972</td>\n",
              "      <td>0.953730</td>\n",
              "      <td>0.000298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_030ln10ewn.jpg</td>\n",
              "      <td>0.941781</td>\n",
              "      <td>0.043251</td>\n",
              "      <td>0.014968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_03z57m8xht.jpg</td>\n",
              "      <td>0.999697</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_04ngep1w4b.jpg</td>\n",
              "      <td>0.998063</td>\n",
              "      <td>0.001426</td>\n",
              "      <td>0.000511</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5c8b01e-bc6e-429e-84cb-0b5e632e4184')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c5c8b01e-bc6e-429e-84cb-0b5e632e4184 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c5c8b01e-bc6e-429e-84cb-0b5e632e4184');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "score : 0.22"
      ],
      "metadata": {
        "id": "58NR3cVZqNPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# resnext50_32x4d (Timm ver.)"
      ],
      "metadata": {
        "id": "46ir4JU1qOZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myResNeXt50(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(myResNeXt50, self).__init__()\n",
        "    self.model_ft = create_model('resnext50_32x4d', pretrained=True)\n",
        "    \n",
        "    num_ftrs = self.model_ft.fc.in_features\n",
        "    self.model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.model_ft(x)\n",
        "      return out"
      ],
      "metadata": {
        "id": "FUfCW7pyqdfJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnext50_tf = myResNeXt50(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1WbF4PZqyJn",
        "outputId": "a282540c-956b-4eca-bcd0-c41a47a2d9db"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnext50_32x4d_a1h-0146ab0a.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d_a1h-0146ab0a.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "train_loader, val_loader, test_loader = make_data_loader_with_aug(batch_size=128, size=224)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = optim.Adam(resnext50_tf.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "MODEL_DIR = '/content/gdrive/MyDrive/ML/MRDC-competition/models'\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.00001, path=os.path.join(MODEL_DIR, \"resnext50_aug_checkpoint.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M3whCowq2PJ",
        "outputId": "2207483c-871b-430d-c38d-be1169711706"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "1232\n",
            "1233\n",
            "1234\n",
            "1235\n",
            "1236\n",
            "1237\n",
            "1238\n",
            "1239\n",
            "1240\n",
            "1241\n",
            "1242\n",
            "1243\n",
            "1244\n",
            "1245\n",
            "1246\n",
            "1247\n",
            "1248\n",
            "1249\n",
            "1250\n",
            "1251\n",
            "1252\n",
            "1253\n",
            "1254\n",
            "1255\n",
            "1256\n",
            "1257\n",
            "1258\n",
            "1259\n",
            "1260\n",
            "1261\n",
            "1262\n",
            "1263\n",
            "1264\n",
            "1265\n",
            "1266\n",
            "1267\n",
            "1268\n",
            "1269\n",
            "1270\n",
            "1271\n",
            "1272\n",
            "1273\n",
            "1274\n",
            "1275\n",
            "1276\n",
            "1277\n",
            "1278\n",
            "1279\n",
            "1280\n",
            "1281\n",
            "1282\n",
            "1283\n",
            "1284\n",
            "1285\n",
            "1286\n",
            "1287\n",
            "1288\n",
            "1289\n",
            "1290\n",
            "1291\n",
            "1292\n",
            "1293\n",
            "1294\n",
            "1295\n",
            "1296\n",
            "1297\n",
            "1298\n",
            "1299\n",
            "1300\n",
            "1301\n",
            "1302\n",
            "1303\n",
            "1304\n",
            "1305\n",
            "1306\n",
            "1307\n",
            "1308\n",
            "1309\n",
            "1310\n",
            "1311\n",
            "1312\n",
            "1313\n",
            "1314\n",
            "1315\n",
            "1316\n",
            "1317\n",
            "1318\n",
            "1319\n",
            "1320\n",
            "1321\n",
            "1322\n",
            "1323\n",
            "1324\n",
            "1325\n",
            "1326\n",
            "1327\n",
            "1328\n",
            "1329\n",
            "1330\n",
            "1331\n",
            "1332\n",
            "1333\n",
            "1334\n",
            "1335\n",
            "1336\n",
            "1337\n",
            "1338\n",
            "1339\n",
            "1340\n",
            "1341\n",
            "1342\n",
            "1343\n",
            "1344\n",
            "1345\n",
            "1346\n",
            "1347\n",
            "1348\n",
            "1349\n",
            "1350\n",
            "1351\n",
            "1352\n",
            "1353\n",
            "1354\n",
            "1355\n",
            "1356\n",
            "1357\n",
            "1358\n",
            "1359\n",
            "1360\n",
            "1361\n",
            "1362\n",
            "1363\n",
            "1364\n",
            "1365\n",
            "1366\n",
            "1367\n",
            "1368\n",
            "1369\n",
            "1370\n",
            "1371\n",
            "1372\n",
            "1373\n",
            "1374\n",
            "1375\n",
            "1376\n",
            "1377\n",
            "1378\n",
            "1379\n",
            "1380\n",
            "1381\n",
            "1382\n",
            "1383\n",
            "1384\n",
            "1385\n",
            "1386\n",
            "1387\n",
            "1388\n",
            "1389\n",
            "1390\n",
            "1391\n",
            "1392\n",
            "1393\n",
            "1394\n",
            "1395\n",
            "1396\n",
            "1397\n",
            "1398\n",
            "1399\n",
            "1400\n",
            "1401\n",
            "1402\n",
            "1403\n",
            "1404\n",
            "1405\n",
            "1406\n",
            "1407\n",
            "1408\n",
            "1409\n",
            "1410\n",
            "1411\n",
            "1412\n",
            "1413\n",
            "1414\n",
            "1415\n",
            "1416\n",
            "1417\n",
            "1418\n",
            "1419\n",
            "1420\n",
            "1421\n",
            "1422\n",
            "1423\n",
            "1424\n",
            "1425\n",
            "1426\n",
            "1427\n",
            "1428\n",
            "1429\n",
            "1430\n",
            "1431\n",
            "1432\n",
            "1433\n",
            "1434\n",
            "1435\n",
            "1436\n",
            "1437\n",
            "1438\n",
            "1439\n",
            "1440\n",
            "1441\n",
            "1442\n",
            "1443\n",
            "1444\n",
            "1445\n",
            "1446\n",
            "1447\n",
            "1448\n",
            "1449\n",
            "1450\n",
            "1451\n",
            "1452\n",
            "1453\n",
            "1454\n",
            "1455\n",
            "1456\n",
            "1457\n",
            "1458\n",
            "1459\n",
            "1460\n",
            "1461\n",
            "1462\n",
            "1463\n",
            "1464\n",
            "1465\n",
            "1466\n",
            "1467\n",
            "1468\n",
            "1469\n",
            "1470\n",
            "1471\n",
            "1472\n",
            "1473\n",
            "1474\n",
            "1475\n",
            "1476\n",
            "1477\n",
            "1478\n",
            "1479\n",
            "1480\n",
            "1481\n",
            "1482\n",
            "1483\n",
            "1484\n",
            "1485\n",
            "1486\n",
            "1487\n",
            "1488\n",
            "1489\n",
            "1490\n",
            "1491\n",
            "1492\n",
            "1493\n",
            "1494\n",
            "1495\n",
            "1496\n",
            "1497\n",
            "1498\n",
            "1499\n",
            "1500\n",
            "1501\n",
            "1502\n",
            "1503\n",
            "1504\n",
            "1505\n",
            "1506\n",
            "1507\n",
            "1508\n",
            "1509\n",
            "1510\n",
            "1511\n",
            "1512\n",
            "1513\n",
            "1514\n",
            "1515\n",
            "1516\n",
            "1517\n",
            "1518\n",
            "1519\n",
            "1520\n",
            "1521\n",
            "1522\n",
            "1523\n",
            "1524\n",
            "1525\n",
            "1526\n",
            "1527\n",
            "1528\n",
            "1529\n",
            "1530\n",
            "1531\n",
            "1532\n",
            "1533\n",
            "1534\n",
            "1535\n",
            "1536\n",
            "1537\n",
            "1538\n",
            "1539\n",
            "1540\n",
            "1541\n",
            "1542\n",
            "1543\n",
            "1544\n",
            "1545\n",
            "1546\n",
            "1547\n",
            "1548\n",
            "1549\n",
            "1550\n",
            "1551\n",
            "1552\n",
            "1553\n",
            "1554\n",
            "1555\n",
            "1556\n",
            "1557\n",
            "1558\n",
            "1559\n",
            "1560\n",
            "1561\n",
            "1562\n",
            "1563\n",
            "1564\n",
            "1565\n",
            "1566\n",
            "1567\n",
            "1568\n",
            "1569\n",
            "1570\n",
            "1571\n",
            "1572\n",
            "1573\n",
            "1574\n",
            "1575\n",
            "1576\n",
            "1577\n",
            "1578\n",
            "1579\n",
            "1580\n",
            "1581\n",
            "1582\n",
            "1583\n",
            "1584\n",
            "1585\n",
            "1586\n",
            "1587\n",
            "1588\n",
            "1589\n",
            "1590\n",
            "1591\n",
            "1592\n",
            "1593\n",
            "1594\n",
            "1595\n",
            "1596\n",
            "1597\n",
            "1598\n",
            "1599\n",
            "1600\n",
            "1601\n",
            "1602\n",
            "1603\n",
            "1604\n",
            "1605\n",
            "1606\n",
            "1607\n",
            "1608\n",
            "1609\n",
            "1610\n",
            "1611\n",
            "1612\n",
            "1613\n",
            "1614\n",
            "1615\n",
            "1616\n",
            "1617\n",
            "1618\n",
            "1619\n",
            "1620\n",
            "1621\n",
            "1622\n",
            "1623\n",
            "1624\n",
            "1625\n",
            "1626\n",
            "1627\n",
            "1628\n",
            "1629\n",
            "1630\n",
            "1631\n",
            "1632\n",
            "1633\n",
            "1634\n",
            "1635\n",
            "1636\n",
            "1637\n",
            "1638\n",
            "1639\n",
            "1640\n",
            "1641\n",
            "1642\n",
            "1643\n",
            "1644\n",
            "1645\n",
            "1646\n",
            "1647\n",
            "1648\n",
            "1649\n",
            "1650\n",
            "1651\n",
            "1652\n",
            "1653\n",
            "1654\n",
            "1655\n",
            "1656\n",
            "1657\n",
            "1658\n",
            "1659\n",
            "1660\n",
            "1661\n",
            "1662\n",
            "1663\n",
            "1664\n",
            "1665\n",
            "1666\n",
            "1667\n",
            "1668\n",
            "1669\n",
            "1670\n",
            "1671\n",
            "1672\n",
            "1673\n",
            "1674\n",
            "1675\n",
            "1676\n",
            "1677\n",
            "1678\n",
            "1679\n",
            "1680\n",
            "1681\n",
            "1682\n",
            "1683\n",
            "1684\n",
            "1685\n",
            "1686\n",
            "1687\n",
            "1688\n",
            "1689\n",
            "1690\n",
            "1691\n",
            "1692\n",
            "1693\n",
            "1694\n",
            "1695\n",
            "1696\n",
            "1697\n",
            "1698\n",
            "1699\n",
            "1700\n",
            "1701\n",
            "1702\n",
            "1703\n",
            "1704\n",
            "1705\n",
            "1706\n",
            "1707\n",
            "1708\n",
            "1709\n",
            "1710\n",
            "1711\n",
            "1712\n",
            "1713\n",
            "1714\n",
            "1715\n",
            "1716\n",
            "1717\n",
            "1718\n",
            "1719\n",
            "1720\n",
            "1721\n",
            "1722\n",
            "1723\n",
            "1724\n",
            "1725\n",
            "1726\n",
            "1727\n",
            "1728\n",
            "1729\n",
            "1730\n",
            "1731\n",
            "1732\n",
            "1733\n",
            "1734\n",
            "1735\n",
            "1736\n",
            "1737\n",
            "1738\n",
            "1739\n",
            "1740\n",
            "1741\n",
            "1742\n",
            "1743\n",
            "1744\n",
            "1745\n",
            "1746\n",
            "1747\n",
            "1748\n",
            "1749\n",
            "1750\n",
            "1751\n",
            "1752\n",
            "1753\n",
            "1754\n",
            "1755\n",
            "1756\n",
            "1757\n",
            "1758\n",
            "1759\n",
            "1760\n",
            "1761\n",
            "1762\n",
            "1763\n",
            "1764\n",
            "1765\n",
            "1766\n",
            "1767\n",
            "1768\n",
            "1769\n",
            "1770\n",
            "1771\n",
            "1772\n",
            "1773\n",
            "1774\n",
            "1775\n",
            "1776\n",
            "1777\n",
            "1778\n",
            "1779\n",
            "1780\n",
            "1781\n",
            "1782\n",
            "1783\n",
            "1784\n",
            "1785\n",
            "1786\n",
            "1787\n",
            "1788\n",
            "1789\n",
            "1790\n",
            "1791\n",
            "1792\n",
            "1793\n",
            "1794\n",
            "1795\n",
            "1796\n",
            "1797\n",
            "1798\n",
            "1799\n",
            "1800\n",
            "1801\n",
            "1802\n",
            "1803\n",
            "1804\n",
            "1805\n",
            "1806\n",
            "1807\n",
            "1808\n",
            "1809\n",
            "1810\n",
            "1811\n",
            "1812\n",
            "1813\n",
            "1814\n",
            "1815\n",
            "1816\n",
            "1817\n",
            "1818\n",
            "1819\n",
            "1820\n",
            "1821\n",
            "1822\n",
            "1823\n",
            "1824\n",
            "1825\n",
            "1826\n",
            "1827\n",
            "1828\n",
            "1829\n",
            "1830\n",
            "1831\n",
            "1832\n",
            "1833\n",
            "1834\n",
            "1835\n",
            "1836\n",
            "1837\n",
            "1838\n",
            "1839\n",
            "1840\n",
            "1841\n",
            "1842\n",
            "1843\n",
            "1844\n",
            "1845\n",
            "1846\n",
            "1847\n",
            "1848\n",
            "1849\n",
            "1850\n",
            "1851\n",
            "1852\n",
            "1853\n",
            "1854\n",
            "1855\n",
            "1856\n",
            "1857\n",
            "1858\n",
            "1859\n",
            "1860\n",
            "1861\n",
            "1862\n",
            "1863\n",
            "1864\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1868\n",
            "1869\n",
            "1870\n",
            "1871\n",
            "1872\n",
            "1873\n",
            "1874\n",
            "1875\n",
            "1876\n",
            "1877\n",
            "1878\n",
            "1879\n",
            "1880\n",
            "1881\n",
            "1882\n",
            "1883\n",
            "1884\n",
            "1885\n",
            "1886\n",
            "1887\n",
            "1888\n",
            "1889\n",
            "1890\n",
            "1891\n",
            "1892\n",
            "1893\n",
            "1894\n",
            "1895\n",
            "1896\n",
            "1897\n",
            "1898\n",
            "1899\n",
            "1900\n",
            "1901\n",
            "1902\n",
            "1903\n",
            "1904\n",
            "1905\n",
            "1906\n",
            "1907\n",
            "1908\n",
            "1909\n",
            "1910\n",
            "1911\n",
            "1912\n",
            "1913\n",
            "1914\n",
            "1915\n",
            "1916\n",
            "1917\n",
            "1918\n",
            "1919\n",
            "1920\n",
            "1921\n",
            "1922\n",
            "1923\n",
            "1924\n",
            "1925\n",
            "1926\n",
            "1927\n",
            "1928\n",
            "1929\n",
            "1930\n",
            "1931\n",
            "1932\n",
            "1933\n",
            "1934\n",
            "1935\n",
            "1936\n",
            "1937\n",
            "1938\n",
            "1939\n",
            "1940\n",
            "1941\n",
            "1942\n",
            "1943\n",
            "1944\n",
            "1945\n",
            "1946\n",
            "1947\n",
            "1948\n",
            "1949\n",
            "1950\n",
            "1951\n",
            "1952\n",
            "1953\n",
            "1954\n",
            "1955\n",
            "1956\n",
            "1957\n",
            "1958\n",
            "1959\n",
            "1960\n",
            "1961\n",
            "1962\n",
            "1963\n",
            "1964\n",
            "1965\n",
            "1966\n",
            "1967\n",
            "1968\n",
            "1969\n",
            "1970\n",
            "1971\n",
            "1972\n",
            "1973\n",
            "1974\n",
            "1975\n",
            "1976\n",
            "1977\n",
            "1978\n",
            "1979\n",
            "1980\n",
            "1981\n",
            "1982\n",
            "1983\n",
            "1984\n",
            "1985\n",
            "1986\n",
            "1987\n",
            "1988\n",
            "1989\n",
            "1990\n",
            "1991\n",
            "1992\n",
            "1993\n",
            "1994\n",
            "1995\n",
            "1996\n",
            "1997\n",
            "1998\n",
            "1999\n",
            "2000\n",
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "2025\n",
            "2026\n",
            "2027\n",
            "2028\n",
            "2029\n",
            "2030\n",
            "2031\n",
            "2032\n",
            "2033\n",
            "2034\n",
            "2035\n",
            "2036\n",
            "2037\n",
            "2038\n",
            "2039\n",
            "2040\n",
            "2041\n",
            "2042\n",
            "2043\n",
            "2044\n",
            "2045\n",
            "2046\n",
            "2047\n",
            "2048\n",
            "2049\n",
            "2050\n",
            "2051\n",
            "2052\n",
            "2053\n",
            "2054\n",
            "2055\n",
            "2056\n",
            "2057\n",
            "2058\n",
            "2059\n",
            "2060\n",
            "2061\n",
            "2062\n",
            "2063\n",
            "2064\n",
            "2065\n",
            "2066\n",
            "2067\n",
            "2068\n",
            "2069\n",
            "2070\n",
            "2071\n",
            "2072\n",
            "2073\n",
            "2074\n",
            "2075\n",
            "2076\n",
            "2077\n",
            "2078\n",
            "2079\n",
            "2080\n",
            "2081\n",
            "2082\n",
            "2083\n",
            "2084\n",
            "2085\n",
            "2086\n",
            "2087\n",
            "2088\n",
            "2089\n",
            "2090\n",
            "2091\n",
            "2092\n",
            "2093\n",
            "2094\n",
            "2095\n",
            "2096\n",
            "2097\n",
            "2098\n",
            "2099\n",
            "2100\n",
            "2101\n",
            "2102\n",
            "2103\n",
            "2104\n",
            "2105\n",
            "2106\n",
            "2107\n",
            "2108\n",
            "2109\n",
            "2110\n",
            "2111\n",
            "2112\n",
            "2113\n",
            "2114\n",
            "2115\n",
            "2116\n",
            "2117\n",
            "2118\n",
            "2119\n",
            "2120\n",
            "2121\n",
            "2122\n",
            "2123\n",
            "2124\n",
            "2125\n",
            "2126\n",
            "2127\n",
            "2128\n",
            "2129\n",
            "2130\n",
            "2131\n",
            "2132\n",
            "2133\n",
            "2134\n",
            "2135\n",
            "2136\n",
            "2137\n",
            "2138\n",
            "2139\n",
            "2140\n",
            "2141\n",
            "2142\n",
            "2143\n",
            "2144\n",
            "2145\n",
            "2146\n",
            "2147\n",
            "2148\n",
            "2149\n",
            "2150\n",
            "2151\n",
            "2152\n",
            "2153\n",
            "2154\n",
            "2155\n",
            "2156\n",
            "2157\n",
            "2158\n",
            "2159\n",
            "2160\n",
            "2161\n",
            "2162\n",
            "2163\n",
            "2164\n",
            "2165\n",
            "2166\n",
            "2167\n",
            "2168\n",
            "2169\n",
            "2170\n",
            "2171\n",
            "2172\n",
            "2173\n",
            "2174\n",
            "2175\n",
            "2176\n",
            "2177\n",
            "2178\n",
            "2179\n",
            "2180\n",
            "2181\n",
            "2182\n",
            "2183\n",
            "2184\n",
            "2185\n",
            "2186\n",
            "2187\n",
            "2188\n",
            "2189\n",
            "2190\n",
            "2191\n",
            "2192\n",
            "2193\n",
            "2194\n",
            "2195\n",
            "2196\n",
            "2197\n",
            "2198\n",
            "2199\n",
            "2200\n",
            "2201\n",
            "2202\n",
            "2203\n",
            "2204\n",
            "2205\n",
            "2206\n",
            "2207\n",
            "2208\n",
            "2209\n",
            "2210\n",
            "2211\n",
            "2212\n",
            "2213\n",
            "2214\n",
            "2215\n",
            "2216\n",
            "2217\n",
            "2218\n",
            "2219\n",
            "2220\n",
            "2221\n",
            "2222\n",
            "2223\n",
            "2224\n",
            "2225\n",
            "2226\n",
            "2227\n",
            "2228\n",
            "2229\n",
            "2230\n",
            "2231\n",
            "2232\n",
            "2233\n",
            "2234\n",
            "2235\n",
            "2236\n",
            "2237\n",
            "2238\n",
            "2239\n",
            "2240\n",
            "2241\n",
            "2242\n",
            "2243\n",
            "2244\n",
            "2245\n",
            "2246\n",
            "2247\n",
            "2248\n",
            "2249\n",
            "2250\n",
            "2251\n",
            "2252\n",
            "2253\n",
            "2254\n",
            "2255\n",
            "2256\n",
            "2257\n",
            "2258\n",
            "2259\n",
            "2260\n",
            "2261\n",
            "2262\n",
            "2263\n",
            "2264\n",
            "2265\n",
            "2266\n",
            "2267\n",
            "2268\n",
            "2269\n",
            "2270\n",
            "2271\n",
            "2272\n",
            "2273\n",
            "2274\n",
            "2275\n",
            "2276\n",
            "2277\n",
            "2278\n",
            "2279\n",
            "2280\n",
            "2281\n",
            "2282\n",
            "2283\n",
            "2284\n",
            "2285\n",
            "2286\n",
            "2287\n",
            "2288\n",
            "2289\n",
            "2290\n",
            "2291\n",
            "2292\n",
            "2293\n",
            "2294\n",
            "2295\n",
            "2296\n",
            "2297\n",
            "2298\n",
            "2299\n",
            "2300\n",
            "2301\n",
            "2302\n",
            "2303\n",
            "2304\n",
            "2305\n",
            "2306\n",
            "2307\n",
            "2308\n",
            "2309\n",
            "2310\n",
            "2311\n",
            "2312\n",
            "2313\n",
            "2314\n",
            "2315\n",
            "2316\n",
            "2317\n",
            "2318\n",
            "2319\n",
            "2320\n",
            "2321\n",
            "2322\n",
            "2323\n",
            "2324\n",
            "2325\n",
            "2326\n",
            "2327\n",
            "2328\n",
            "2329\n",
            "2330\n",
            "2331\n",
            "2332\n",
            "2333\n",
            "2334\n",
            "2335\n",
            "2336\n",
            "2337\n",
            "2338\n",
            "2339\n",
            "2340\n",
            "2341\n",
            "2342\n",
            "2343\n",
            "2344\n",
            "2345\n",
            "2346\n",
            "2347\n",
            "2348\n",
            "2349\n",
            "2350\n",
            "2351\n",
            "2352\n",
            "2353\n",
            "2354\n",
            "2355\n",
            "2356\n",
            "2357\n",
            "2358\n",
            "2359\n",
            "2360\n",
            "2361\n",
            "2362\n",
            "2363\n",
            "2364\n",
            "2365\n",
            "2366\n",
            "2367\n",
            "2368\n",
            "2369\n",
            "2370\n",
            "2371\n",
            "2372\n",
            "2373\n",
            "2374\n",
            "2375\n",
            "2376\n",
            "2377\n",
            "2378\n",
            "2379\n",
            "2380\n",
            "2381\n",
            "2382\n",
            "2383\n",
            "2384\n",
            "2385\n",
            "2386\n",
            "2387\n",
            "2388\n",
            "2389\n",
            "2390\n",
            "2391\n",
            "2392\n",
            "2393\n",
            "2394\n",
            "2395\n",
            "2396\n",
            "2397\n",
            "2398\n",
            "2399\n",
            "2400\n",
            "2401\n",
            "2402\n",
            "2403\n",
            "2404\n",
            "2405\n",
            "2406\n",
            "2407\n",
            "2408\n",
            "2409\n",
            "2410\n",
            "2411\n",
            "2412\n",
            "2413\n",
            "2414\n",
            "2415\n",
            "2416\n",
            "2417\n",
            "2418\n",
            "2419\n",
            "2420\n",
            "2421\n",
            "2422\n",
            "2423\n",
            "2424\n",
            "2425\n",
            "2426\n",
            "2427\n",
            "2428\n",
            "2429\n",
            "2430\n",
            "2431\n",
            "2432\n",
            "2433\n",
            "2434\n",
            "2435\n",
            "2436\n",
            "2437\n",
            "2438\n",
            "2439\n",
            "2440\n",
            "2441\n",
            "2442\n",
            "2443\n",
            "2444\n",
            "2445\n",
            "2446\n",
            "2447\n",
            "2448\n",
            "2449\n",
            "2450\n",
            "2451\n",
            "2452\n",
            "2453\n",
            "2454\n",
            "2455\n",
            "2456\n",
            "2457\n",
            "2458\n",
            "2459\n",
            "2460\n",
            "2461\n",
            "2462\n",
            "2463\n",
            "2464\n",
            "2465\n",
            "2466\n",
            "2467\n",
            "2468\n",
            "2469\n",
            "2470\n",
            "2471\n",
            "2472\n",
            "2473\n",
            "2474\n",
            "2475\n",
            "2476\n",
            "2477\n",
            "2478\n",
            "2479\n",
            "2480\n",
            "2481\n",
            "2482\n",
            "2483\n",
            "2484\n",
            "2485\n",
            "2486\n",
            "2487\n",
            "2488\n",
            "2489\n",
            "2490\n",
            "2491\n",
            "2492\n",
            "2493\n",
            "2494\n",
            "2495\n",
            "2496\n",
            "2497\n",
            "2498\n",
            "2499\n",
            "2500\n",
            "2501\n",
            "2502\n",
            "2503\n",
            "2504\n",
            "2505\n",
            "2506\n",
            "2507\n",
            "2508\n",
            "2509\n",
            "2510\n",
            "2511\n",
            "2512\n",
            "2513\n",
            "2514\n",
            "2515\n",
            "2516\n",
            "2517\n",
            "2518\n",
            "2519\n",
            "2520\n",
            "2521\n",
            "2522\n",
            "2523\n",
            "2524\n",
            "2525\n",
            "2526\n",
            "2527\n",
            "2528\n",
            "2529\n",
            "2530\n",
            "2531\n",
            "2532\n",
            "2533\n",
            "2534\n",
            "2535\n",
            "2536\n",
            "2537\n",
            "2538\n",
            "2539\n",
            "2540\n",
            "2541\n",
            "2542\n",
            "2543\n",
            "2544\n",
            "2545\n",
            "2546\n",
            "2547\n",
            "2548\n",
            "2549\n",
            "2550\n",
            "2551\n",
            "2552\n",
            "2553\n",
            "2554\n",
            "2555\n",
            "2556\n",
            "2557\n",
            "2558\n",
            "2559\n",
            "2560\n",
            "2561\n",
            "2562\n",
            "2563\n",
            "2564\n",
            "2565\n",
            "2566\n",
            "2567\n",
            "2568\n",
            "2569\n",
            "2570\n",
            "2571\n",
            "2572\n",
            "2573\n",
            "2574\n",
            "2575\n",
            "2576\n",
            "2577\n",
            "2578\n",
            "2579\n",
            "2580\n",
            "2581\n",
            "2582\n",
            "2583\n",
            "2584\n",
            "2585\n",
            "2586\n",
            "2587\n",
            "2588\n",
            "2589\n",
            "2590\n",
            "2591\n",
            "2592\n",
            "2593\n",
            "2594\n",
            "2595\n",
            "2596\n",
            "2597\n",
            "2598\n",
            "2599\n",
            "2600\n",
            "2601\n",
            "2602\n",
            "2603\n",
            "2604\n",
            "2605\n",
            "2606\n",
            "2607\n",
            "2608\n",
            "2609\n",
            "2610\n",
            "2611\n",
            "2612\n",
            "2613\n",
            "2614\n",
            "2615\n",
            "2616\n",
            "2617\n",
            "2618\n",
            "2619\n",
            "2620\n",
            "2621\n",
            "2622\n",
            "2623\n",
            "2624\n",
            "2625\n",
            "2626\n",
            "2627\n",
            "2628\n",
            "2629\n",
            "2630\n",
            "2631\n",
            "2632\n",
            "2633\n",
            "2634\n",
            "2635\n",
            "2636\n",
            "2637\n",
            "2638\n",
            "2639\n",
            "2640\n",
            "2641\n",
            "2642\n",
            "2643\n",
            "2644\n",
            "2645\n",
            "2646\n",
            "2647\n",
            "2648\n",
            "2649\n",
            "2650\n",
            "2651\n",
            "2652\n",
            "2653\n",
            "2654\n",
            "2655\n",
            "2656\n",
            "2657\n",
            "2658\n",
            "2659\n",
            "2660\n",
            "2661\n",
            "2662\n",
            "2663\n",
            "2664\n",
            "2665\n",
            "2666\n",
            "2667\n",
            "2668\n",
            "2669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnext50_tf, val_label, val_pred = train_model(device, resnext50_tf, train_loader, val_loader, criterion, optimizer, 30, early_stopping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCXN8Aqgq_st",
        "outputId": "6585bd35-6855-4fb4-de2c-bf8e43cfc616"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "train Loss: 0.8997 Acc: 0.6527\n",
            "val Loss: 0.5708 Acc: 0.7976\n",
            "Validation loss decreased (inf --> 0.570780).  Saving model ...\n",
            "Epoch 2/30\n",
            "----------\n",
            "train Loss: 0.3345 Acc: 0.8909\n",
            "val Loss: 0.3099 Acc: 0.8801\n",
            "Validation loss decreased (0.570780 --> 0.309892).  Saving model ...\n",
            "Epoch 3/30\n",
            "----------\n",
            "train Loss: 0.0942 Acc: 0.9751\n",
            "val Loss: 0.1488 Acc: 0.9486\n",
            "Validation loss decreased (0.309892 --> 0.148824).  Saving model ...\n",
            "Epoch 4/30\n",
            "----------\n",
            "train Loss: 0.0295 Acc: 0.9936\n",
            "val Loss: 0.1312 Acc: 0.9486\n",
            "Validation loss decreased (0.148824 --> 0.131169).  Saving model ...\n",
            "Epoch 5/30\n",
            "----------\n",
            "train Loss: 0.0327 Acc: 0.9898\n",
            "val Loss: 0.1618 Acc: 0.9443\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 6/30\n",
            "----------\n",
            "train Loss: 0.0232 Acc: 0.9933\n",
            "val Loss: 0.2180 Acc: 0.9390\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 7/30\n",
            "----------\n",
            "train Loss: 0.0388 Acc: 0.9893\n",
            "val Loss: 0.1710 Acc: 0.9368\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 8/30\n",
            "----------\n",
            "train Loss: 0.0340 Acc: 0.9898\n",
            "val Loss: 0.1463 Acc: 0.9475\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch 9/30\n",
            "----------\n",
            "train Loss: 0.0297 Acc: 0.9914\n",
            "val Loss: 0.1377 Acc: 0.9615\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = test_model(device, resnext50_tf, test_loader)"
      ],
      "metadata": {
        "id": "oPoiMzIHtGze"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss.loc[:, ['blast', 'brown', 'healthy']] = test_pred\n",
        "ss.to_csv(\"result_resnext50_timm_ver_aug.csv\", index=False)\n",
        "ss.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IvGg05oCtK9L",
        "outputId": "941f2f75-9c65-4cc5-a30e-211a4a80824b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Image_id     blast     brown   healthy\n",
              "0  id_00vl5wvxq3.jpg  0.999966  0.000020  0.000013\n",
              "1  id_01hu05mtch.jpg  0.000458  0.999233  0.000309\n",
              "2  id_030ln10ewn.jpg  0.618977  0.016132  0.364891\n",
              "3  id_03z57m8xht.jpg  0.999896  0.000080  0.000024\n",
              "4  id_04ngep1w4b.jpg  0.996957  0.001783  0.001260"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da5142f3-fd2f-4f86-97a3-22c9f44b195d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_id</th>\n",
              "      <th>blast</th>\n",
              "      <th>brown</th>\n",
              "      <th>healthy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_00vl5wvxq3.jpg</td>\n",
              "      <td>0.999966</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_01hu05mtch.jpg</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.999233</td>\n",
              "      <td>0.000309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_030ln10ewn.jpg</td>\n",
              "      <td>0.618977</td>\n",
              "      <td>0.016132</td>\n",
              "      <td>0.364891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_03z57m8xht.jpg</td>\n",
              "      <td>0.999896</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_04ngep1w4b.jpg</td>\n",
              "      <td>0.996957</td>\n",
              "      <td>0.001783</td>\n",
              "      <td>0.001260</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da5142f3-fd2f-4f86-97a3-22c9f44b195d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da5142f3-fd2f-4f86-97a3-22c9f44b195d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da5142f3-fd2f-4f86-97a3-22c9f44b195d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "score : 0.4 (엥?)"
      ],
      "metadata": {
        "id": "BGFSTlAutb8O"
      }
    }
  ]
}